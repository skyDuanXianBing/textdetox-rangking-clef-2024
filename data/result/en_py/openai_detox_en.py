from openai import OpenAI
import re
import json
import pandas as pd
import os
import time

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
script_dir = os.path.dirname(os.path.abspath(__file__))

# è¯»å–é…ç½®æ–‡ä»¶
config_path = os.path.join(script_dir, "config.json")
with open(config_path, "r") as f:
    config = json.load(f)

# å…¨å±€å˜é‡å­˜å‚¨å½“å‰ä½¿ç”¨çš„æ¨¡å‹é…ç½®
current_model_config = None
current_model_name = None


def extract_json_from_string(input_string):
    """ä»å­—ç¬¦ä¸²ä¸­æå–JSONå†…å®¹"""
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… JSON éƒ¨åˆ†
    json_match = re.search(r"\{.*\}", input_string, re.DOTALL)

    if json_match:
        json_str = json_match.group(0)
        # å°† JSON å­—ç¬¦ä¸²ä¸­çš„åŒå¼•å·æ›¿æ¢ä¸ºå•å¼•å·
        json_str = json_str.replace('""', '"')
        # å°† JSON å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—å…¸
        json_dict = json.loads(json_str)
        return json_dict
    else:
        print(f"æ— æ•ˆçš„jsonå­—æ®µ{input_string}")
        raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ JSON å†…å®¹")


def get_client(model_config):
    """æ ¹æ®æ¨¡å‹é…ç½®åˆ›å»ºOpenAIå®¢æˆ·ç«¯"""
    return OpenAI(
        api_key=model_config["apiKey"],
        base_url=model_config["baseUrl"]
    )


def normalize_score(score):
    """
    å°†è¯„åˆ†æ ‡å‡†åŒ–ä¸º0ã€0.5ã€1ä¸‰ä¸ªå€¼
    """
    if score >= 0.75:
        return 1.0
    elif score >= 0.25:
        return 0.5
    else:
        return 0.0


def calculate_joint_score(sta, cs, fs):
    """
    è®¡ç®—è”åˆåˆ†æ•°(Joint Score)
    æ ¹æ®è®ºæ–‡ï¼ŒJSæ˜¯STAã€CSã€FSçš„ç»¼åˆè¯„åˆ†
    ä½¿ç”¨ä¸‰é¡¹ç›¸ä¹˜çš„æ–¹å¼è®¡ç®—
    """
    return sta * cs * fs


def etd_few_shot_evaluation(toxic_sentence, neutral_sentence, retry_count=5, initial_delay=30):
    """
    ä½¿ç”¨ETDæ¡†æ¶è¿›è¡Œå°‘æ ·æœ¬æ–‡æœ¬è§£æ¯’è¯„ä¼°
    åŒ…å«ç¤ºä¾‹æ¥å¸®åŠ©æ¨¡å‹ç†è§£è¯„ä¼°æ ‡å‡†
    """
    delay = initial_delay
    client = get_client(current_model_config)

    for attempt in range(retry_count):
        try:
            response = client.chat.completions.create(
                model=current_model_config["model"],
                temperature=current_model_config["temperature"],
                messages=[
                    {
                        "role": "user",
                        "content": f"""You are now an expert evaluator for text detoxification tasks. Please evaluate the following text detoxification results according to the ETD evaluation framework.

<Background>
Text detoxification aims to rewrite sentences containing toxic content into neutral sentences while preserving the original meaning. You need to evaluate from three dimensions: Style Transfer Accuracy, Content Similarity, and Fluency.
</Background>

<Task Description>
Please evaluate the given toxic text and neutral text pair from the following three dimensions:
1. Style Transfer Accuracy (STA): Evaluate whether the neutral text successfully removes toxic content
2. Content Similarity (CS): Evaluate whether the neutral text maintains the core meaning of the original text
3. Fluency (FS): Evaluate the language quality and readability of the neutral text
</Task Description>

<Evaluation Criteria>
1. Style Transfer Accuracy (STA): Only 0, 0.5, 1 scores allowed
   - 1.0: Completely removes toxic content, language is completely neutral
   - 0.5: Partially removes toxic content, still has some toxic expressions
   - 0.0: Almost no toxic content removed

2. Content Similarity (CS): Only 0, 0.5, 1 scores allowed
   - 1.0: Completely preserves original meaning, no loss of core information
   - 0.5: Partially preserves original meaning, some information loss
   - 0.0: Almost no preservation of original meaning

3. Fluency (FS): Only 0, 0.5, 1 scores allowed
   - 1.0: Language is completely fluent, no grammatical errors
   - 0.5: Understandable but has some grammatical or expression issues
   - 0.0: Difficult to understand, serious grammatical errors
</Evaluation Criteria>

<Task Examples>
Example 1:
Toxic text: "any of us who did half this shit would be locked the fuck up already."
Neutral text: "any of us who did half of these actions would likely be incarcerated already."
Evaluation result: {{"STA": 1.0, "CS": 1.0, "FS": 1.0, "j_score": 1.0, "reasoning": "Successfully converted vulgar language to formal expression, completely preserved original meaning, language is fluent and natural"}}

Example 2:
Toxic text: "are they liars or just clueless?"
Neutral text: "are they dishonest or simply unaware?"
Evaluation result: {{"STA": 1.0, "CS": 1.0, "FS": 1.0, "j_score": 1.0, "reasoning": "Replaced aggressive vocabulary with neutral words, meaning remains consistent, expression is more polite"}}

Example 3:
Toxic text: "cretins like this rarely care about facts."
Neutral text: "people with such behavior rarely care about facts."
Evaluation result: {{"STA": 1.0, "CS": 0.5, "FS": 1.0, "j_score": 0.83, "reasoning": "Removed insulting vocabulary and replaced with neutral description, basically preserved original meaning but slightly generalized"}}
</Task Examples>

<Input Data Format>
Toxic text: {toxic_sentence}
Neutral text: {neutral_sentence}
</Input Data Format>

<Output Data Format>
Please strictly output the evaluation results in the following JSON format:

```json
{{
"STA": number, // Style Transfer Accuracy score (can only be 0, 0.5, or 1)
"CS": number,  // Content Similarity score (can only be 0, 0.5, or 1)
"FS": number,  // Fluency score (can only be 0, 0.5, or 1)
"j_score": number,  // Joint score (automatically calculated average)
"reasoning": "string" // Detailed evaluation reasoning, explaining the basis for each score
}}
```
</Output Data Format>

Please carefully analyze the given text pair, refer to the evaluation standards in the examples, and provide objective and accurate evaluation results.""",
                    }
                ],
            )

            result = extract_json_from_string(response.choices[0].message.content)

            # æ ‡å‡†åŒ–è¯„åˆ†ä¸º0ã€0.5ã€1ä¸‰ä¸ªå€¼
            if 'STA' in result:
                result['STA'] = normalize_score(result['STA'])
            if 'CS' in result:
                result['CS'] = normalize_score(result['CS'])
            if 'FS' in result:
                result['FS'] = normalize_score(result['FS'])

            # è®¡ç®—è”åˆåˆ†æ•°
            if 'STA' in result and 'CS' in result and 'FS' in result:
                result['j_score'] = calculate_joint_score(result['STA'], result['CS'], result['FS'])

            return result

        except Exception as e:
            error_str = str(e)
            print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")

            # æ£€æŸ¥ä¸åŒç±»å‹çš„é”™è¯¯å¹¶é‡‡ç”¨ä¸åŒçš„é‡è¯•ç­–ç•¥
            if any(keyword in error_str.lower() for keyword in ["429", "rate limit", "è´Ÿè½½å·²é¥±å’Œ", "too many requests"]):
                if attempt < retry_count - 1:
                    print(f"æ£€æµ‹åˆ°é™æµé”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    delay = min(delay * 1.5, 300)  # æœ€å¤§ç­‰å¾…5åˆ†é’Ÿ
                else:
                    print("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒå¤„ç†")
                    return None
            elif any(keyword in error_str.lower() for keyword in ["timeout", "connection", "network"]):
                if attempt < retry_count - 1:
                    print(f"æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾… {min(delay//2, 15)} ç§’åé‡è¯•...")
                    time.sleep(min(delay//2, 15))
                else:
                    print("ç½‘ç»œé”™è¯¯é‡è¯•å¤±è´¥")
                    return None
            else:
                # å…¶ä»–é”™è¯¯ï¼Œè¾ƒçŸ­ç­‰å¾…æ—¶é—´
                if attempt < retry_count - 1:
                    print(f"å…¶ä»–é”™è¯¯ï¼Œç­‰å¾… 10 ç§’åé‡è¯•...")
                    time.sleep(10)
                else:
                    print("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                    return None


def etd_zero_shot_evaluation(toxic_sentence, neutral_sentence, retry_count=5, initial_delay=30):
    """
    ä½¿ç”¨ETDæ¡†æ¶è¿›è¡Œé›¶æ ·æœ¬æ–‡æœ¬è§£æ¯’è¯„ä¼°
    æ ¹æ®è®ºæ–‡è¦æ±‚è¯„ä¼°STAã€CSã€FSä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡
    """
    delay = initial_delay
    client = get_client(current_model_config)
    
    for attempt in range(retry_count):
        try:
            response = client.chat.completions.create(
                model=current_model_config["model"],
                temperature=current_model_config["temperature"],
                messages=[
                    {
                        "role": "user",
                        "content": f"""You are now an expert evaluator for text detoxification tasks. Please evaluate the following text detoxification results according to the ETD evaluation framework.

<Background>
Text detoxification aims to rewrite sentences containing toxic content into neutral sentences while preserving the original meaning. You need to evaluate from three dimensions: Style Transfer Accuracy, Content Similarity, and Fluency.
</Background>

<Task Description>
Please evaluate the given toxic text and neutral text pair from the following three dimensions:
1. Style Transfer Accuracy (STA): Evaluate whether the neutral text successfully removes toxic content
2. Content Similarity (CS): Evaluate whether the neutral text maintains the core meaning of the original text
3. Fluency (FS): Evaluate the language quality and readability of the neutral text
</Task Description>

<Evaluation Criteria>
1. Style Transfer Accuracy (STA): Only 0, 0.5, 1 scores allowed
   - 1.0: Completely removes toxic content, language is completely neutral
   - 0.5: Partially removes toxic content, still has some toxic expressions
   - 0.0: Almost no toxic content removed

2. Content Similarity (CS): Only 0, 0.5, 1 scores allowed
   - 1.0: Completely preserves original meaning, no loss of core information
   - 0.5: Partially preserves original meaning, some information loss
   - 0.0: Almost no preservation of original meaning

3. Fluency (FS): Only 0, 0.5, 1 scores allowed
   - 1.0: Language is completely fluent, no grammatical errors
   - 0.5: Understandable but has some grammatical or expression issues
   - 0.0: Difficult to understand, serious grammatical errors
</Evaluation Criteria>

<Input Data Format>
Toxic text: {toxic_sentence}
Neutral text: {neutral_sentence}
</Input Data Format>

<Output Data Format>
Please strictly output the evaluation results in the following JSON format:

```json
{{
"STA": number, // Style Transfer Accuracy score (can only be 0, 0.5, or 1)
"CS": number,  // Content Similarity score (can only be 0, 0.5, or 1)
"FS": number,  // Fluency score (can only be 0, 0.5, or 1)
"j_score": number,  // Joint score (automatically calculated average)
"reasoning": "string" // Detailed evaluation reasoning, explaining the basis for each score
}}
```
</Output Data Format>

Please carefully analyze the given text pair and provide objective and accurate evaluation results.""",
                    }
                ],
            )
            
            result = extract_json_from_string(response.choices[0].message.content)

            # æ ‡å‡†åŒ–è¯„åˆ†ä¸º0ã€0.5ã€1ä¸‰ä¸ªå€¼
            if 'STA' in result:
                result['STA'] = normalize_score(result['STA'])
            if 'CS' in result:
                result['CS'] = normalize_score(result['CS'])
            if 'FS' in result:
                result['FS'] = normalize_score(result['FS'])

            # è®¡ç®—è”åˆåˆ†æ•°
            if 'STA' in result and 'CS' in result and 'FS' in result:
                result['j_score'] = calculate_joint_score(result['STA'], result['CS'], result['FS'])

            return result
            
        except Exception as e:
            error_str = str(e)
            print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
            
            # æ£€æŸ¥ä¸åŒç±»å‹çš„é”™è¯¯å¹¶é‡‡ç”¨ä¸åŒçš„é‡è¯•ç­–ç•¥
            if any(keyword in error_str.lower() for keyword in ["429", "rate limit", "è´Ÿè½½å·²é¥±å’Œ", "too many requests"]):
                if attempt < retry_count - 1:
                    print(f"æ£€æµ‹åˆ°é™æµé”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    delay = min(delay * 1.5, 300)  # æœ€å¤§ç­‰å¾…5åˆ†é’Ÿ
                else:
                    print("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒå¤„ç†")
                    return None
            elif any(keyword in error_str.lower() for keyword in ["timeout", "connection", "network"]):
                if attempt < retry_count - 1:
                    print(f"æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾… {min(delay//2, 15)} ç§’åé‡è¯•...")
                    time.sleep(min(delay//2, 15))
                else:
                    print("ç½‘ç»œé”™è¯¯é‡è¯•å¤±è´¥")
                    return None
            else:
                # å…¶ä»–é”™è¯¯ï¼Œè¾ƒçŸ­ç­‰å¾…æ—¶é—´
                if attempt < retry_count - 1:
                    print(f"å…¶ä»–é”™è¯¯ï¼Œç­‰å¾… 10 ç§’åé‡è¯•...")
                    time.sleep(10)
                else:
                    print("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                    return None


def find_missing_data(df_original, df_current):
    """æ‰¾å‡ºæœªå¤„ç†çš„æ•°æ®ç´¢å¼•"""
    # è·å–å·²å¤„ç†çš„å¥å­å¯¹
    processed_pairs = set()
    for _, row in df_current.iterrows():
        processed_pairs.add((row['toxic_sentence'], row['neutral_sentence']))
    
    # æ‰¾å‡ºæœªå¤„ç†çš„æ•°æ®ç´¢å¼•
    missing_indices = []
    for idx, row in df_original.iterrows():
        pair = (row['toxic_sentence'], row['neutral_sentence'])
        if pair not in processed_pairs:
            missing_indices.append(idx)
    
    return missing_indices


def get_matched_teams():
    """è·å–ä¸æ¦œå•åŒ¹é…çš„å›¢é˜Ÿåˆ—è¡¨"""
    # æ¦œå•ä¸­çš„å›¢é˜Ÿåç§°
    leaderboard_teams = [
        "Team MarSanAI", "Team nlp_enjoyers", "Team cake", "mT5", "gangopsa",
        "Team SINAI", "delete", "Team Iron Autobots", "LanaKlitotekhnis", 
        "Anastasia1706", "ZhongyuLuo", "cocount", "backtranslation", "etomoscow",
        "cointegrated", "dkenco", "FD", "duplicate", "Team SmurfCat", "Imeribal",
        "nikita.sushko", "VitalyProtasov", "erehulka", "SomethingAwful", 
        "mareksuppa", "kofeinix", "Yekaterina29", "AlekseevArtem", "Team NLPunks",
        "pavelshtykov", "gleb.shnshn", "Volodimirich", "ansafronov", "MOOsipenko", "mkrisnai"
    ]
    
    # å»é™¤é‡å¤
    leaderboard_teams = list(set(leaderboard_teams))
    
    # è·å–æ‰€æœ‰å¯ç”¨çš„TSVæ–‡ä»¶
    all_tsv_files = [f for f in os.listdir(script_dir) if f.endswith('_english.tsv')]
    available_teams = [f.replace('_english.tsv', '') for f in all_tsv_files]
    
    # åˆ›å»ºåŒ¹é…æ˜ å°„
    name_mappings = {
        "Team MarSanAI": "Team MarSan_AI",
        "Team SINAI": "Team_SINAI", 
        "mT5": "mt5_baseline",
        "delete": "delete_baseline",
        "backtranslation": "backtranslation_baseline",
    }
    
    # æ‰¾å‡ºåŒ¹é…çš„å›¢é˜Ÿ
    matched_teams = []
    for lb_team in leaderboard_teams:
        # ç›´æ¥åŒ¹é…
        if lb_team in available_teams:
            matched_teams.append(lb_team)
        # é€šè¿‡æ˜ å°„åŒ¹é…
        elif lb_team in name_mappings and name_mappings[lb_team] in available_teams:
            matched_teams.append(name_mappings[lb_team])
        # æ¨¡ç³ŠåŒ¹é…
        else:
            lb_normalized = lb_team.lower().replace(" ", "").replace("_", "")
            for av_team in available_teams:
                av_normalized = av_team.lower().replace(" ", "").replace("_", "")
                if lb_normalized == av_normalized:
                    matched_teams.append(av_team)
                    break
    
    return matched_teams


def main():
    """ä¸»å‡½æ•°"""
    global current_model_config, current_model_name
    
    # æ˜¾ç¤ºå¯ç”¨çš„æ¨¡å‹
    available_models = list(config.keys())
    print("ğŸ¤– å¯ç”¨çš„æ¨¡å‹:")
    print("=" * 50)
    for i, model_name in enumerate(available_models, 1):
        model_info = config[model_name]
        print(f"{i:2d}. {model_name} ({model_info['model']})")
    
    # è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    while True:
        try:
            choice = input(f"\nè¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹ (1-{len(available_models)}) æˆ–è¾“å…¥ 'all' è¿è¡Œæ‰€æœ‰æ¨¡å‹: ").strip()
            if choice.lower() == 'all':
                selected_models = available_models
                break
            else:
                model_idx = int(choice) - 1
                if 0 <= model_idx < len(available_models):
                    selected_models = [available_models[model_idx]]
                    break
                else:
                    print(f"è¯·è¾“å…¥ 1-{len(available_models)} ä¹‹é—´çš„æ•°å­—æˆ– 'all'")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—æˆ– 'all'")
    
    # è·å–ä¸æ¦œå•åŒ¹é…çš„å›¢é˜Ÿ
    matched_teams = get_matched_teams()
    
    if not matched_teams:
        print("æœªæ‰¾åˆ°ä»»ä½•ä¸æ¦œå•åŒ¹é…çš„å›¢é˜Ÿæ–‡ä»¶ï¼")
        return
    
    # ç”Ÿæˆå¯¹åº”çš„TSVæ–‡ä»¶åˆ—è¡¨
    matched_tsv_files = [f"{team}_english.tsv" for team in matched_teams]
    
    print(f"\nğŸ¯ æ‰¾åˆ° {len(matched_tsv_files)} ä¸ªåŒ¹é…çš„å›¢é˜Ÿæ–‡ä»¶:")
    for i, file in enumerate(matched_tsv_files, 1):
        team_name = file.replace('_english.tsv', '')
        print(f"{i:2d}. {team_name}")
    
    # è®©ç”¨æˆ·é€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶
    while True:
        try:
            choice = input(f"\nè¯·é€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶ (1-{len(matched_tsv_files)}) æˆ–è¾“å…¥ 'all' å¤„ç†æ‰€æœ‰åŒ¹é…å›¢é˜Ÿ: ").strip()
            if choice.lower() == 'all':
                selected_files = matched_tsv_files
                break
            else:
                file_idx = int(choice) - 1
                if 0 <= file_idx < len(matched_tsv_files):
                    selected_files = [matched_tsv_files[file_idx]]
                    break
                else:
                    print(f"è¯·è¾“å…¥ 1-{len(matched_tsv_files)} ä¹‹é—´çš„æ•°å­—æˆ– 'all'")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—æˆ– 'all'")
    
    # è®©ç”¨æˆ·é€‰æ‹©è¯„ä¼°æ–¹æ³•
    print("\nè¯·é€‰æ‹©è¯„ä¼°æ–¹æ³•:")
    print("1. Zero-shot è¯„ä¼° (ä¸æä¾›ç¤ºä¾‹)")
    print("2. Few-shot è¯„ä¼° (æä¾›ç¤ºä¾‹)")

    while True:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
        if choice == "1":
            evaluation_method = "zero_shot"
            evaluation_func = etd_zero_shot_evaluation
            break
        elif choice == "2":
            evaluation_method = "few_shot"
            evaluation_func = etd_few_shot_evaluation
            break
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")

    print(f"å·²é€‰æ‹© {evaluation_method} è¯„ä¼°æ–¹æ³•")
    
    # å¤„ç†æ¯ä¸ªé€‰ä¸­çš„æ¨¡å‹
    for model_idx, model_name in enumerate(selected_models, 1):
        current_model_name = model_name
        current_model_config = config[model_name]
        
        print(f"\nğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name} ({current_model_config['model']})")
        print(f"æ¨¡å‹è¿›åº¦: {model_idx}/{len(selected_models)}")
        
        # å¤„ç†æ¯ä¸ªé€‰ä¸­çš„æ–‡ä»¶
        print(f"\nğŸš€ å¼€å§‹å¤„ç† {len(selected_files)} ä¸ªåŒ¹é…å›¢é˜Ÿçš„æ–‡ä»¶")
        for i, tsv_file in enumerate(selected_files, 1):
            print(f"\n{'='*60}")
            print(f"å¤„ç†è¿›åº¦: {i}/{len(selected_files)} - {tsv_file} (æ¨¡å‹: {model_name})")
            print(f"{'='*60}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            data_path = os.path.join(script_dir, tsv_file)
            if not os.path.exists(data_path):
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {tsv_file}")
                continue
                
            # è¯»å–æ•°æ®æ–‡ä»¶
            try:
                df_c = pd.read_csv(data_path, sep="\t")
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶ {tsv_file} å¤±è´¥: {e}")
                continue
            
            # è·å–é˜Ÿä¼åç§°ï¼ˆå»æ‰_english.tsvåç¼€ï¼‰
            team_name = tsv_file.replace('_english.tsv', '')
            
            # åˆ›å»ºç»“æœæ–‡ä»¶å¤¹å’Œæ–‡ä»¶å - æ ¹æ®æ¨¡å‹åç§°åˆ›å»ºä¸åŒçš„æ–‡ä»¶å¤¹
            parent_dir = os.path.dirname(script_dir)  # è·å–ä¸Šçº§ç›®å½• (data/result)
            llm_evolution_dir = os.path.join(parent_dir, "llm_evolution")
            results_dir = os.path.join(llm_evolution_dir, f"{team_name}_en", model_name)
            os.makedirs(results_dir, exist_ok=True)
            
            if evaluation_method == "zero_shot":
                results_filename = os.path.join(results_dir, f"etd_zero_shot_results_en_ENPrompt_{model_name}.csv")
            else:
                results_filename = os.path.join(results_dir, f"etd_few_shot_results_en_ENPrompt_{model_name}.csv")
            
            process_file(df_c, results_filename, evaluation_func, team_name, model_name)
    
    print(f"\nğŸ‰ æ‰€æœ‰æ¨¡å‹å’Œå›¢é˜Ÿå¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {os.path.join(os.path.dirname(script_dir), 'llm_evolution')}")


def process_file(df_c, results_filename, evaluation_func, team_name, model_name=None):
    """å¤„ç†å•ä¸ªæ–‡ä»¶çš„å‡½æ•°"""
    print(f"\nå¤„ç†é˜Ÿä¼: {team_name}")
    if model_name:
        print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
    print(f"æ•°æ®é‡: {len(df_c)} æ¡")

    # åˆ›å»ºç»“æœæ–‡ä»¶è·¯å¾„
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²æœ‰ç»“æœæ–‡ä»¶
    if os.path.exists(results_filename):
        df_current = pd.read_csv(results_filename)
        print(f"å‘ç°å·²æœ‰ç»“æœæ–‡ä»¶ï¼Œå·²å¤„ç† {len(df_current)} æ¡æ•°æ®")
        
        # æ‰¾å‡ºæœªå¤„ç†çš„æ•°æ®
        missing_indices = find_missing_data(df_c, df_current)
        if not missing_indices:
            print(f"é˜Ÿä¼ {team_name} çš„æ‰€æœ‰æ•°æ®éƒ½å·²å¤„ç†å®Œæˆï¼")
            return
        
        print(f"è¿˜æœ‰ {len(missing_indices)} æ¡æ•°æ®éœ€è¦å¤„ç†")
        start_indices = missing_indices
    else:
        # åˆ›å»ºæ–°çš„ç»“æœæ–‡ä»¶
        df_current = pd.DataFrame(
            columns=[
                "toxic_sentence",
                "neutral_sentence", 
                "STA",  # é£æ ¼è¿ç§»å‡†ç¡®æ€§
                "CS",   # å†…å®¹ä¿ç•™åº¦
                "FS",   # æµç•…æ€§
                "j_score",   # è”åˆåˆ†æ•°
                "reasoning",  # è¯„ä¼°ç†ç”±
            ]
        )
        df_current.to_csv(results_filename, index=False)
        start_indices = list(range(len(df_c)))
        print(f"åˆ›å»ºæ–°çš„ç»“æœæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç† {len(df_c)} æ¡æ•°æ®...")
    
    # å¤„ç†æ•°æ®
    success_count = 0
    new_results = []
    
    for i, idx in enumerate(start_indices):
        if idx < len(df_c):
            row = df_c.iloc[idx]
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {idx+1} æ¡æ•°æ® ({i+1}/{len(start_indices)})...")
            print(f"æœ‰æ¯’å¥å­: {row['toxic_sentence'][:50]}{'...' if len(row['toxic_sentence']) > 50 else ''}")
            print(f"ä¸­æ€§å¥å­: {row['neutral_sentence'][:50]}{'...' if len(row['neutral_sentence']) > 50 else ''}")
            
            result = evaluation_func(row["toxic_sentence"], row["neutral_sentence"])
            
            if result:
                new_row = {
                    "toxic_sentence": row["toxic_sentence"],
                    "neutral_sentence": row["neutral_sentence"],
                    "STA": result["STA"],
                    "CS": result["CS"],
                    "FS": result["FS"],
                    "j_score": result["j_score"],
                    "reasoning": result["reasoning"]
                }
                new_results.append(new_row)
                success_count += 1
                print(f"âœ… ç¬¬ {idx+1} æ¡æ•°æ®å¤„ç†æˆåŠŸ (æˆåŠŸ: {success_count}/{i+1})")
                print(f"ç»“æœ: STA={result['STA']}, CS={result['CS']}, FS={result['FS']}, j_score={result['j_score']:.2f}")
                
                # æ¯å¤„ç†æˆåŠŸ3æ¡æ•°æ®å°±ä¿å­˜ä¸€æ¬¡ï¼Œé¿å…æ•°æ®ä¸¢å¤±
                if len(new_results) % 3 == 0:
                    df_new = pd.DataFrame(new_results)
                    df_updated = pd.concat([df_current, df_new], ignore_index=True)
                    df_updated.to_csv(results_filename, index=False)
                    print(f"ğŸ’¾ å·²ä¿å­˜ {len(new_results)} æ¡æ–°æ•°æ®åˆ°æ–‡ä»¶")
                    df_current = df_updated  # æ›´æ–°å½“å‰æ•°æ®æ¡†
                    new_results = []  # æ¸…ç©ºä¸´æ—¶ç»“æœ
                    
            else:
                print(f"âŒ ç¬¬ {idx+1} æ¡æ•°æ®å¤„ç†å¤±è´¥")
                
            # æ¯å¤„ç†å®Œä¸€æ¡æ•°æ®åçŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i < len(start_indices) - 1:  # ä¸æ˜¯æœ€åä¸€æ¡æ•°æ®
                time.sleep(2)
    
    # ä¿å­˜å‰©ä½™çš„ç»“æœ
    if new_results:
        df_new = pd.DataFrame(new_results)
        df_updated = pd.concat([df_current, df_new], ignore_index=True)
        df_updated.to_csv(results_filename, index=False)
        print(f"\nğŸ’¾ æœ€ç»ˆä¿å­˜å®Œæˆ")
    
    # é‡æ–°è¯»å–æœ€ç»ˆç»“æœå¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    df_final = pd.read_csv(results_filename)
    
    print(f"\nğŸ“Š é˜Ÿä¼ {team_name} å¤„ç†æ€»ç»“:")
    if model_name:
        print(f"   - ä½¿ç”¨æ¨¡å‹: {model_name}")
    print(f"   - å°è¯•å¤„ç†: {len(start_indices)} æ¡æ•°æ®")
    print(f"   - æˆåŠŸå¤„ç†: {success_count} æ¡æ•°æ®")
    print(f"   - å¤±è´¥æ•°é‡: {len(start_indices) - success_count} æ¡æ•°æ®")
    print(f"   - æˆåŠŸç‡: {success_count/len(start_indices)*100:.1f}%")
    print(f"   - æ–‡ä»¶ä¸­æ€»æ•°æ®é‡: {len(df_final)} æ¡")
    
    # è®¡ç®—å„æŒ‡æ ‡çš„å¹³å‡åˆ†
    if len(df_final) > 0:
        print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœç»Ÿè®¡:")
        print(f"   - STAå¹³å‡åˆ†: {df_final['STA'].mean():.2f}")
        print(f"   - CSå¹³å‡åˆ†: {df_final['CS'].mean():.2f}")
        print(f"   - FSå¹³å‡åˆ†: {df_final['FS'].mean():.2f}")
        print(f"   - JSå¹³å‡åˆ†: {df_final['j_score'].mean():.2f}")
    
    if len(df_final) < len(df_c):
        remaining = len(df_c) - len(df_final)
        print(f"   - ä»æœ‰ {remaining} æ¡æ•°æ®æœªå¤„ç†ï¼Œå¯ä»¥å†æ¬¡è¿è¡Œæ­¤è„šæœ¬ç»§ç»­å¤„ç†")
    else:
        model_info = f" (æ¨¡å‹: {model_name})" if model_name else ""
        print(f"ğŸ‰ é˜Ÿä¼ {team_name}{model_info} çš„æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()
