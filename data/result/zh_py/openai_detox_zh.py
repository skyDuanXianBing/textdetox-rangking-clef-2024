#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¸­æ–‡æ–‡æœ¬å»æ¯’åŒ–LLMè¯„ä¼°è„šæœ¬
åŸºäºETDè¯„ä¼°æ¡†æ¶ï¼Œä½¿ç”¨DeepSeek APIå¯¹ä¸­æ–‡å»æ¯’åŒ–ç»“æœè¿›è¡Œè¯„ä¼°
è¯„ä¼°æŒ‡æ ‡ï¼šSTA (é£æ ¼è¿ç§»å‡†ç¡®æ€§)ã€CS (å†…å®¹ç›¸ä¼¼åº¦)ã€FS (æµç•…æ€§)
"""

import json
import os
import pandas as pd
import re
import time
from openai import OpenAI

# è¯»å–é…ç½®æ–‡ä»¶
script_dir = os.path.dirname(os.path.abspath(__file__))
config_path = os.path.join(script_dir, "..", "en_py", "config.json")

with open(config_path, 'r', encoding='utf-8') as f:
    config = json.load(f)

# å…¨å±€å˜é‡å­˜å‚¨å½“å‰ä½¿ç”¨çš„æ¨¡å‹é…ç½®
current_model_config = None
current_model_name = None


def get_client(model_config):
    """æ ¹æ®æ¨¡å‹é…ç½®åˆ›å»ºOpenAIå®¢æˆ·ç«¯"""
    return OpenAI(
        api_key=model_config["apiKey"],
        base_url=model_config["baseUrl"]
    )


def normalize_score(score):
    """
    å°†è¯„åˆ†æ ‡å‡†åŒ–ä¸º0ã€0.5ã€1ä¸‰ä¸ªå€¼
    """
    if score >= 0.75:
        return 1.0
    elif score >= 0.25:
        return 0.5
    else:
        return 0.0


def calculate_joint_score(sta, cs, fs):
    """
    è®¡ç®—è”åˆåˆ†æ•°(Joint Score)
    æ ¹æ®è®ºæ–‡ï¼ŒJSæ˜¯STAã€CSã€FSçš„ç»¼åˆè¯„åˆ†
    ä½¿ç”¨ä¸‰é¡¹ç›¸ä¹˜çš„æ–¹å¼è®¡ç®—
    """
    return sta * cs * fs


def extract_json_from_string(text):
    """
    ä»å­—ç¬¦ä¸²ä¸­æå–JSONå¯¹è±¡
    """
    try:
        # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå­—ç¬¦ä¸²
        return json.loads(text)
    except json.JSONDecodeError:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾JSONå—
        json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
        matches = re.findall(json_pattern, text, re.DOTALL)
        
        for match in matches:
            try:
                return json.loads(match)
            except json.JSONDecodeError:
                continue
        
        # å¦‚æœä»ç„¶å¤±è´¥ï¼Œå°è¯•æå–å…³é”®ä¿¡æ¯
        sta_match = re.search(r'"STA":\s*([0-9.]+)', text)
        cs_match = re.search(r'"CS":\s*([0-9.]+)', text)
        fs_match = re.search(r'"FS":\s*([0-9.]+)', text)
        reasoning_match = re.search(r'"reasoning":\s*"([^"]*)"', text)
        
        if sta_match and cs_match and fs_match:
            return {
                "STA": float(sta_match.group(1)),
                "CS": float(cs_match.group(1)),
                "FS": float(fs_match.group(1)),
                "reasoning": reasoning_match.group(1) if reasoning_match else "æ— æ³•æå–æ¨ç†è¿‡ç¨‹"
            }
        
        raise ValueError(f"æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„JSON: {text}")


def etd_few_shot_evaluation_zh(toxic_sentence, neutral_sentence, retry_count=5, initial_delay=30):
    """
    ä½¿ç”¨ETDæ¡†æ¶è¿›è¡Œä¸­æ–‡å°‘æ ·æœ¬æ–‡æœ¬è§£æ¯’è¯„ä¼°
    åŒ…å«ç¤ºä¾‹æ¥å¸®åŠ©æ¨¡å‹ç†è§£è¯„ä¼°æ ‡å‡†
    """
    delay = initial_delay
    client = get_client(current_model_config)

    for attempt in range(retry_count):
        try:
            response = client.chat.completions.create(
                model=current_model_config["model"],
                temperature=current_model_config["temperature"],
                messages=[
                    {
                        "role": "user",
                        "content": f"""ä½ ç°åœ¨æ˜¯ä¸€ä¸ªä¸­æ–‡æ–‡æœ¬å»æ¯’åŒ–ä»»åŠ¡çš„ä¸“ä¸šè¯„ä¼°å‘˜ã€‚è¯·æ ¹æ®ETDè¯„ä¼°æ¡†æ¶å¯¹ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬å»æ¯’åŒ–ç»“æœè¿›è¡Œè¯„ä¼°ã€‚

<èƒŒæ™¯ä»‹ç»>
æ–‡æœ¬å»æ¯’åŒ–æ—¨åœ¨å°†åŒ…å«æœ‰æ¯’å†…å®¹çš„å¥å­é‡å†™ä¸ºä¸­æ€§å¥å­ï¼ŒåŒæ—¶ä¿æŒåŸå§‹å«ä¹‰ã€‚ä½ éœ€è¦ä»ä¸‰ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ï¼šé£æ ¼è¿ç§»å‡†ç¡®æ€§ã€å†…å®¹ç›¸ä¼¼åº¦å’Œæµç•…æ€§ã€‚
</èƒŒæ™¯ä»‹ç»>

<ä»»åŠ¡æè¿°>
è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦è¯„ä¼°ç»™å®šçš„æœ‰æ¯’æ–‡æœ¬å’Œä¸­æ€§æ–‡æœ¬å¯¹ï¼š
1. é£æ ¼è¿ç§»å‡†ç¡®æ€§ (STA)ï¼šè¯„ä¼°ä¸­æ€§æ–‡æœ¬æ˜¯å¦æˆåŠŸå»é™¤æœ‰æ¯’å†…å®¹
2. å†…å®¹ç›¸ä¼¼åº¦ (CS)ï¼šè¯„ä¼°ä¸­æ€§æ–‡æœ¬æ˜¯å¦ä¿æŒåŸæ–‡çš„æ ¸å¿ƒå«ä¹‰
3. æµç•…æ€§ (FS)ï¼šè¯„ä¼°ä¸­æ€§æ–‡æœ¬çš„è¯­è¨€è´¨é‡å’Œå¯è¯»æ€§
</ä»»åŠ¡æè¿°>

**è¯„ä¼°æ ‡å‡†ï¼š**

1. **STA (Style Transfer Accuracy) - é£æ ¼è¿ç§»å‡†ç¡®æ€§ (0-1åˆ†)**
   - 1.0åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬å®Œå…¨æ¶ˆé™¤äº†æœ‰æ¯’å†…å®¹ï¼Œæ²¡æœ‰ä»»ä½•æ”»å‡»æ€§ã€æ­§è§†æ€§æˆ–æœ‰å®³è¯­è¨€
   - 0.5åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬å¤§éƒ¨åˆ†æ¶ˆé™¤äº†æœ‰æ¯’å†…å®¹ï¼Œä½†ä»æœ‰è½»å¾®çš„è´Ÿé¢è¡¨è¾¾
   - 0.0åˆ†ï¼šå»æ¯’åŒ–å¤±è´¥ï¼Œä»åŒ…å«æ˜æ˜¾çš„æœ‰æ¯’ã€æ”»å‡»æ€§æˆ–æœ‰å®³å†…å®¹

2. **CS (Content Similarity) - å†…å®¹ç›¸ä¼¼åº¦ (0-1åˆ†)**
   - 1.0åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬å®Œç¾ä¿æŒäº†åŸæ–‡çš„æ ¸å¿ƒä¿¡æ¯å’Œè¯­ä¹‰
   - 0.5åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬ä¿æŒäº†å¤§éƒ¨åˆ†åŸæ–‡ä¿¡æ¯ï¼Œä½†æœ‰ä¸€äº›ç»†èŠ‚ä¸¢å¤±
   - 0.0åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬ä¸åŸæ–‡è¯­ä¹‰å·®å¼‚å¾ˆå¤§ï¼Œæ ¸å¿ƒä¿¡æ¯ä¸¥é‡ä¸¢å¤±

3. **FS (Fluency Score) - æµç•…æ€§ (0-1åˆ†)**
   - 1.0åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬è¯­æ³•æ­£ç¡®ï¼Œè¡¨è¾¾è‡ªç„¶æµç•…ï¼Œç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯
   - 0.5åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬åŸºæœ¬æµç•…ï¼Œä½†æœ‰è½»å¾®çš„è¯­æ³•é”™è¯¯æˆ–è¡¨è¾¾ä¸è‡ªç„¶
   - 0.0åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬è¯­æ³•é”™è¯¯æ˜æ˜¾ï¼Œè¡¨è¾¾ä¸æµç•…æˆ–éš¾ä»¥ç†è§£

<ä»»åŠ¡ç¤ºä¾‹>
ç¤ºä¾‹1:
æœ‰æ¯’æ–‡æœ¬ï¼š"è¿™äº›ç™½ç—´æ ¹æœ¬ä¸æ‡‚è£…æ‡‚ï¼Œç®€ç›´æ˜¯æµªè´¹æ—¶é—´ã€‚"
å»æ¯’æ–‡æœ¬ï¼š"è¿™äº›äººå¯èƒ½å¯¹æ­¤ä¸å¤ªäº†è§£ï¼Œè®¨è®ºèµ·æ¥æ•ˆç‡ä¸é«˜ã€‚"
è¯„ä¼°ç»“æœï¼š{{"STA": 1.0, "CS": 1.0, "FS": 1.0, "j_score": 1.0, "reasoning": "æˆåŠŸå°†ä¾®è¾±æ€§è¯æ±‡è½¬æ¢ä¸ºä¸­æ€§è¡¨è¾¾ï¼Œå®Œå…¨ä¿æŒäº†åŸæ„ï¼Œè¯­è¨€è‡ªç„¶æµç•…"}}

ç¤ºä¾‹2:
æœ‰æ¯’æ–‡æœ¬ï¼š"é‚£ä¸ªå‚»å­åˆåœ¨èƒ¡è¯´å…«é“äº†ã€‚"
å»æ¯’æ–‡æœ¬ï¼š"é‚£ä¸ªäººåˆåœ¨å‘è¡¨ä¸å½“è¨€è®ºäº†ã€‚"
è¯„ä¼°ç»“æœï¼š{{"STA": 1.0, "CS": 1.0, "FS": 1.0, "j_score": 1.0, "reasoning": "å»é™¤äº†ä¾®è¾±æ€§ç§°å‘¼ï¼Œç”¨ä¸­æ€§è¯æ±‡æ›¿ä»£ï¼Œæ„æ€ä¿æŒä¸€è‡´ï¼Œè¡¨è¾¾æ›´åŠ ç¤¼è²Œ"}}

ç¤ºä¾‹3:
æœ‰æ¯’æ–‡æœ¬ï¼š"è¿™ç¾¤åºŸç‰©ä»€ä¹ˆéƒ½åšä¸å¥½ã€‚"
å»æ¯’æ–‡æœ¬ï¼š"è¿™äº›äººåœ¨è¿™æ–¹é¢è¡¨ç°ä¸ä½³ã€‚"
è¯„ä¼°ç»“æœï¼š{{"STA": 1.0, "CS": 0.5, "FS": 1.0, "j_score": 0.5, "reasoning": "å»é™¤äº†ä¾®è¾±æ€§è¯æ±‡å¹¶ç”¨ä¸­æ€§æè¿°æ›¿ä»£ï¼ŒåŸºæœ¬ä¿æŒäº†åŸæ„ä½†ç¨æ˜¾æ³›åŒ–ï¼Œè¡¨è¾¾æµç•…"}}
</ä»»åŠ¡ç¤ºä¾‹>

**å¾…è¯„ä¼°æ–‡æœ¬å¯¹ï¼š**
æœ‰æ¯’æ–‡æœ¬ï¼š{toxic_sentence}
å»æ¯’æ–‡æœ¬ï¼š{neutral_sentence}

è¯·ä»”ç»†åˆ†æç»™å®šçš„æ–‡æœ¬å¯¹ï¼Œå‚è€ƒç¤ºä¾‹ä¸­çš„è¯„ä¼°æ ‡å‡†ï¼Œæä¾›å®¢è§‚å‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
    "STA": è¯„åˆ†å€¼,
    "CS": è¯„åˆ†å€¼, 
    "FS": è¯„åˆ†å€¼,
    "j_score": è”åˆåˆ†æ•°,
    "reasoning": "è¯¦ç»†çš„è¯„ä¼°ç†ç”±ï¼Œè§£é‡Šå„é¡¹è¯„åˆ†çš„ä¾æ®"
}}""",
                    }
                ],
            )
            
            result = extract_json_from_string(response.choices[0].message.content)

            # æ ‡å‡†åŒ–è¯„åˆ†ä¸º0ã€0.5ã€1ä¸‰ä¸ªå€¼
            if 'STA' in result:
                result['STA'] = normalize_score(result['STA'])
            if 'CS' in result:
                result['CS'] = normalize_score(result['CS'])
            if 'FS' in result:
                result['FS'] = normalize_score(result['FS'])

            # è®¡ç®—è”åˆåˆ†æ•°
            if 'STA' in result and 'CS' in result and 'FS' in result:
                result['j_score'] = calculate_joint_score(result['STA'], result['CS'], result['FS'])

            return result
            
        except Exception as e:
            error_str = str(e)
            print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
            
            # æ£€æŸ¥ä¸åŒç±»å‹çš„é”™è¯¯å¹¶é‡‡ç”¨ä¸åŒçš„é‡è¯•ç­–ç•¥
            if any(keyword in error_str.lower() for keyword in ["429", "rate limit", "è´Ÿè½½å·²é¥±å’Œ", "too many requests"]):
                if attempt < retry_count - 1:
                    print(f"æ£€æµ‹åˆ°é™æµé”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    delay = min(delay * 1.5, 300)  # æœ€å¤§ç­‰å¾…5åˆ†é’Ÿ
                else:
                    print("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒå¤„ç†")
                    return None
            elif any(keyword in error_str.lower() for keyword in ["timeout", "connection", "network"]):
                if attempt < retry_count - 1:
                    print(f"æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾… {min(delay//2, 15)} ç§’åé‡è¯•...")
                    time.sleep(min(delay//2, 15))
                else:
                    print("ç½‘ç»œé”™è¯¯é‡è¯•å¤±è´¥")
                    return None
            else:
                # å…¶ä»–é”™è¯¯ï¼Œè¾ƒçŸ­ç­‰å¾…æ—¶é—´
                if attempt < retry_count - 1:
                    print(f"å…¶ä»–é”™è¯¯ï¼Œç­‰å¾… 10 ç§’åé‡è¯•...")
                    time.sleep(10)
                else:
                    print("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                    return None


def etd_zero_shot_evaluation_zh(toxic_sentence, neutral_sentence, retry_count=5, initial_delay=30):
    """
    ä½¿ç”¨ETDæ¡†æ¶è¿›è¡Œä¸­æ–‡é›¶æ ·æœ¬æ–‡æœ¬è§£æ¯’è¯„ä¼°
    æ ¹æ®è®ºæ–‡è¦æ±‚è¯„ä¼°STAã€CSã€FSä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡
    """
    delay = initial_delay
    client = get_client(current_model_config)

    for attempt in range(retry_count):
        try:
            response = client.chat.completions.create(
                model=current_model_config["model"],
                temperature=current_model_config["temperature"],
                messages=[
                    {
                        "role": "user",
                        "content": f"""ä½ ç°åœ¨æ˜¯ä¸€ä¸ªä¸­æ–‡æ–‡æœ¬å»æ¯’åŒ–ä»»åŠ¡çš„ä¸“ä¸šè¯„ä¼°å‘˜ã€‚è¯·æ ¹æ®ETDè¯„ä¼°æ¡†æ¶å¯¹ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬å»æ¯’åŒ–ç»“æœè¿›è¡Œè¯„ä¼°ã€‚

**è¯„ä¼°æ ‡å‡†ï¼š**

1. **STA (Style Transfer Accuracy) - é£æ ¼è¿ç§»å‡†ç¡®æ€§ (0-1åˆ†)**
   - 1.0åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬å®Œå…¨æ¶ˆé™¤äº†æœ‰æ¯’å†…å®¹ï¼Œæ²¡æœ‰ä»»ä½•æ”»å‡»æ€§ã€æ­§è§†æ€§æˆ–æœ‰å®³è¯­è¨€
   - 0.5åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬å¤§éƒ¨åˆ†æ¶ˆé™¤äº†æœ‰æ¯’å†…å®¹ï¼Œä½†ä»æœ‰è½»å¾®çš„è´Ÿé¢è¡¨è¾¾
   - 0.0åˆ†ï¼šå»æ¯’åŒ–å¤±è´¥ï¼Œä»åŒ…å«æ˜æ˜¾çš„æœ‰æ¯’ã€æ”»å‡»æ€§æˆ–æœ‰å®³å†…å®¹

2. **CS (Content Similarity) - å†…å®¹ç›¸ä¼¼åº¦ (0-1åˆ†)**
   - 1.0åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬å®Œç¾ä¿æŒäº†åŸæ–‡çš„æ ¸å¿ƒä¿¡æ¯å’Œè¯­ä¹‰
   - 0.5åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬ä¿æŒäº†å¤§éƒ¨åˆ†åŸæ–‡ä¿¡æ¯ï¼Œä½†æœ‰ä¸€äº›ç»†èŠ‚ä¸¢å¤±
   - 0.0åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬ä¸åŸæ–‡è¯­ä¹‰å·®å¼‚å¾ˆå¤§ï¼Œæ ¸å¿ƒä¿¡æ¯ä¸¥é‡ä¸¢å¤±

3. **FS (Fluency Score) - æµç•…æ€§ (0-1åˆ†)**
   - 1.0åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬è¯­æ³•æ­£ç¡®ï¼Œè¡¨è¾¾è‡ªç„¶æµç•…ï¼Œç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯
   - 0.5åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬åŸºæœ¬æµç•…ï¼Œä½†æœ‰è½»å¾®çš„è¯­æ³•é”™è¯¯æˆ–è¡¨è¾¾ä¸è‡ªç„¶
   - 0.0åˆ†ï¼šå»æ¯’åŒ–åçš„æ–‡æœ¬è¯­æ³•é”™è¯¯æ˜æ˜¾ï¼Œè¡¨è¾¾ä¸æµç•…æˆ–éš¾ä»¥ç†è§£

**å¾…è¯„ä¼°æ–‡æœ¬å¯¹ï¼š**
åŸå§‹æœ‰æ¯’æ–‡æœ¬ï¼š{toxic_sentence}
å»æ¯’åŒ–æ–‡æœ¬ï¼š{neutral_sentence}

è¯·ä»”ç»†åˆ†æç»™å®šçš„æ–‡æœ¬å¯¹ï¼Œæä¾›å®¢è§‚å‡†ç¡®çš„è¯„ä¼°ç»“æœã€‚

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
{{
    "STA": è¯„åˆ†å€¼,
    "CS": è¯„åˆ†å€¼, 
    "FS": è¯„åˆ†å€¼,
    "reasoning": "è¯¦ç»†çš„è¯„ä¼°ç†ç”±ï¼Œè§£é‡Šå„é¡¹è¯„åˆ†çš„ä¾æ®"
}}""",
                    }
                ],
            )
            
            result = extract_json_from_string(response.choices[0].message.content)

            # æ ‡å‡†åŒ–è¯„åˆ†ä¸º0ã€0.5ã€1ä¸‰ä¸ªå€¼
            if 'STA' in result:
                result['STA'] = normalize_score(result['STA'])
            if 'CS' in result:
                result['CS'] = normalize_score(result['CS'])
            if 'FS' in result:
                result['FS'] = normalize_score(result['FS'])

            # è®¡ç®—è”åˆåˆ†æ•°
            if 'STA' in result and 'CS' in result and 'FS' in result:
                result['j_score'] = calculate_joint_score(result['STA'], result['CS'], result['FS'])

            return result
            
        except Exception as e:
            error_str = str(e)
            print(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
            
            # æ£€æŸ¥ä¸åŒç±»å‹çš„é”™è¯¯å¹¶é‡‡ç”¨ä¸åŒçš„é‡è¯•ç­–ç•¥
            if any(keyword in error_str.lower() for keyword in ["429", "rate limit", "è´Ÿè½½å·²é¥±å’Œ", "too many requests"]):
                if attempt < retry_count - 1:
                    print(f"æ£€æµ‹åˆ°é™æµé”™è¯¯ï¼Œç­‰å¾… {delay} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    delay = min(delay * 1.5, 300)  # æœ€å¤§ç­‰å¾…5åˆ†é’Ÿ
                else:
                    print("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒå¤„ç†")
                    return None
            elif any(keyword in error_str.lower() for keyword in ["timeout", "connection", "network"]):
                if attempt < retry_count - 1:
                    print(f"æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾… {min(delay//2, 15)} ç§’åé‡è¯•...")
                    time.sleep(min(delay//2, 15))
                else:
                    print("ç½‘ç»œé”™è¯¯é‡è¯•å¤±è´¥")
                    return None
            else:
                # å…¶ä»–é”™è¯¯ï¼Œè¾ƒçŸ­ç­‰å¾…æ—¶é—´
                if attempt < retry_count - 1:
                    print(f"å…¶ä»–é”™è¯¯ï¼Œç­‰å¾… 10 ç§’åé‡è¯•...")
                    time.sleep(10)
                else:
                    print("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                    return None


def load_chinese_test_data_for_team(team_name: str):
    """
    human_evaluation ä¸‹ Chinese.tsvï¼‰
    è¿”å› DataFrameï¼ŒåŒ…å« toxic_sentence ä¸ neutral_sentence ä¸¤åˆ—ã€‚
    """
    base_dir = os.path.join(script_dir, "..", "..", "human_evaluation")
    # å…¼å®¹ Team_SINAI ä¸ Team SINAIã€baseline å‘½å
    candidate_dirs = [
        team_name,
        team_name.replace("_", " "),
        team_name.replace(" ", "_"),
        team_name.replace("baseline", ""),
        team_name.replace("_baseline", ""),
    ]
    for cand in candidate_dirs:
        team_dir = os.path.join(base_dir, cand)
        chinese_tsv = os.path.join(team_dir, "Chinese.tsv")
        if os.path.exists(chinese_tsv):
            try:
                df = pd.read_csv(chinese_tsv, sep='\t')
                # è§„èŒƒåˆ—å
                cols = {c.lower(): c for c in df.columns}
                tox_col = 'toxic_sentence' if 'toxic_sentence' in cols else cols.get('toxic_sentence', 'toxic_sentence')
                neu_col = 'neutral_sentence' if 'neutral_sentence' in cols else cols.get('neutral_sentence', 'neutral_sentence')
                # è‹¥åŸè¡¨å¤´ä¸ºé©¼å³°æˆ–å…¶å®ƒå¤§å°å†™ï¼Œç›´æ¥æ˜ å°„
                if tox_col not in df.columns and 'toxic_sentence' in [c.lower() for c in df.columns]:
                    for c in df.columns:
                        if c.lower() == 'toxic_sentence':
                            tox_col = c
                if neu_col not in df.columns and 'neutral_sentence' in [c.lower() for c in df.columns]:
                    for c in df.columns:
                        if c.lower() == 'neutral_sentence':
                            neu_col = c
                sub = df[[tox_col, neu_col]].rename(columns={tox_col: 'toxic_sentence', neu_col: 'neutral_sentence'})
                print(f"âœ… æˆåŠŸåŠ è½½ {cand} çš„ä¸­æ–‡æ•°æ®: {len(sub)} æ¡è®°å½•")
                return sub
            except Exception as e:
                print(f"âš ï¸  è¯»å– {chinese_tsv} å¤±è´¥: {e}")
                continue
    print(f"âŒ æœªæ‰¾åˆ° {team_name} çš„ Chinese.tsv")
    return None


def get_team_list():
    """
    è·å–éœ€è¦è¯„ä¼°çš„å›¢é˜Ÿåˆ—è¡¨ï¼ˆåŸºäºä¸­æ–‡LLMè¯„ä»·ç»“æœä¸­çš„15ä¸ªæœ‰äººç±»ç»“æœçš„é˜Ÿä¼ï¼‰
    """
    # ä»ä¸­æ–‡LLMè¯„ä»·ç»“æœä¸­è·å–çš„15ä¸ªæœ‰äººç±»ç»“æœçš„é˜Ÿä¼
    zh_teams_with_human_results = [
        "Team cake", "Team_SINAI", "Team NLPunks", "mkrisnai", "SomethingAwful",
        "Team Iron Autobots", "erehulka", "gleb.shnshn", "ZhongyuLuo",
        "delete_baseline", "nikita.sushko", "VitalyProtasov", "Team nlp_enjoyers",
        "mt5_baseline", "backtranslation_baseline"
    ]

    print(f"âœ… å°†è¯„ä¼°ä»¥ä¸‹ {len(zh_teams_with_human_results)} ä¸ªæœ‰äººç±»ç»“æœçš„ä¸­æ–‡å›¢é˜Ÿ:")
    for i, team in enumerate(zh_teams_with_human_results, 1):
        print(f"   {i:2d}. {team}")

    return zh_teams_with_human_results


def process_team_evaluation(team_name, chinese_test_df, evaluation_func, evaluation_method):
    """
    å¤„ç†å•ä¸ªå›¢é˜Ÿçš„è¯„ä¼°
    """
    print(f"\nğŸ”„ å¼€å§‹è¯„ä¼°å›¢é˜Ÿ: {team_name}")
    print("-" * 50)

    # åˆ›å»ºç»“æœç›®å½•ä¸æ–‡ä»¶å - æ ¹æ®æ¨¡å‹åç§°åˆ›å»ºä¸åŒçš„æ–‡ä»¶å¤¹
    parent_dir = os.path.dirname(script_dir)  # data/result
    llm_evolution_dir = os.path.join(parent_dir, "llm_evolution")
    team_results_dir = os.path.join(llm_evolution_dir, f"{team_name}_zh", current_model_name)
    os.makedirs(team_results_dir, exist_ok=True)

    if evaluation_method == "zero_shot":
        results_filename = os.path.join(team_results_dir, f"etd_zero_shot_results_zh_ZHPrompt_{current_model_name}.csv")
    else:
        results_filename = os.path.join(team_results_dir, f"etd_few_shot_results_zh_ZHPrompt_{current_model_name}.csv")

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç»“æœæ–‡ä»¶
    df_current = pd.DataFrame()
    processed_indices = set()

    if os.path.exists(results_filename):
        try:
            df_current = pd.read_csv(results_filename)
            processed_indices = set(df_current.index.tolist())
            print(f"ğŸ“ å‘ç°å·²æœ‰ç»“æœæ–‡ä»¶ï¼Œå·²å¤„ç† {len(df_current)} æ¡æ•°æ®")
        except Exception as e:
            print(f"âš ï¸  è¯»å–å·²æœ‰ç»“æœæ–‡ä»¶å¤±è´¥: {e}")

    # å¤„ç†æ•°æ®
    new_results = []
    success_count = len(df_current)
    total_count = len(chinese_test_df)

    print(f"ğŸ“Š å¼€å§‹å¤„ç† {total_count} æ¡æ•°æ® (å·²å®Œæˆ: {success_count})")

    for i, (idx, row) in enumerate(chinese_test_df.iterrows()):
        if idx in processed_indices:
            continue

        print(f"\nå¤„ç†ç¬¬ {idx+1}/{total_count} æ¡æ•°æ®...")
        print(f"æœ‰æ¯’å¥å­: {row['toxic_sentence'][:50]}...")
        print(f"å»æ¯’å¥å­: {row['neutral_sentence'][:50]}...")

        result = evaluation_func(row['toxic_sentence'], row['neutral_sentence'])

        if result:
            new_row = {
                "toxic_sentence": row["toxic_sentence"],
                "neutral_sentence": row["neutral_sentence"],
                "STA": result["STA"],
                "CS": result["CS"],
                "FS": result["FS"],
                "j_score": result["j_score"],
                "reasoning": result["reasoning"]
            }
            new_results.append(new_row)
            success_count += 1
            print(f"âœ… ç¬¬ {idx+1} æ¡æ•°æ®å¤„ç†æˆåŠŸ (æˆåŠŸ: {success_count}/{i+1})")
            print(f"ç»“æœ: STA={result['STA']}, CS={result['CS']}, FS={result['FS']}, j_score={result['j_score']:.2f}")

            # æ¯å¤„ç†æˆåŠŸ3æ¡æ•°æ®å°±ä¿å­˜ä¸€æ¬¡ï¼Œé¿å…æ•°æ®ä¸¢å¤±
            if len(new_results) % 3 == 0:
                df_new = pd.DataFrame(new_results)
                df_updated = pd.concat([df_current, df_new], ignore_index=True)
                df_updated.to_csv(results_filename, index=False)
                print(f"ğŸ’¾ å·²ä¿å­˜ {len(new_results)} æ¡æ–°æ•°æ®åˆ°æ–‡ä»¶")
                df_current = df_updated  # æ›´æ–°å½“å‰æ•°æ®æ¡†
                new_results = []  # æ¸…ç©ºä¸´æ—¶ç»“æœ
        else:
            print(f"âŒ ç¬¬ {idx+1} æ¡æ•°æ®å¤„ç†å¤±è´¥")

    # ä¿å­˜å‰©ä½™çš„ç»“æœ
    if new_results:
        df_new = pd.DataFrame(new_results)
        df_updated = pd.concat([df_current, df_new], ignore_index=True)
        df_updated.to_csv(results_filename, index=False)
        print(f"ğŸ’¾ æœ€ç»ˆä¿å­˜å®Œæˆï¼Œå…± {len(df_updated)} æ¡æ•°æ®")

    print(f"âœ… å›¢é˜Ÿ {team_name} è¯„ä¼°å®Œæˆ")
    print(f"ğŸ“ ç»“æœä¿å­˜è‡³: {results_filename}")

    return results_filename


def main():
    """ä¸»å‡½æ•°"""
    global current_model_config, current_model_name

    print("ğŸš€ å¼€å§‹ä¸­æ–‡æ–‡æœ¬å»æ¯’åŒ–LLMè¯„ä¼°")
    print("=" * 60)

    # æ˜¾ç¤ºå¯ç”¨çš„æ¨¡å‹
    available_models = list(config.keys())
    print("ğŸ¤– å¯ç”¨çš„æ¨¡å‹:")
    print("=" * 50)
    for i, model_name in enumerate(available_models, 1):
        model_info = config[model_name]
        print(f"{i:2d}. {model_name} ({model_info['model']})")

    # è®©ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    while True:
        try:
            choice = input(f"\nè¯·é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹ (1-{len(available_models)}) æˆ–è¾“å…¥ 'all' è¿è¡Œæ‰€æœ‰æ¨¡å‹: ").strip()
            if choice.lower() == 'all':
                selected_models = available_models
                break
            else:
                model_idx = int(choice) - 1
                if 0 <= model_idx < len(available_models):
                    selected_models = [available_models[model_idx]]
                    break
                else:
                    print(f"è¯·è¾“å…¥ 1-{len(available_models)} ä¹‹é—´çš„æ•°å­—æˆ– 'all'")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—æˆ– 'all'")

    # è·å–å›¢é˜Ÿåˆ—è¡¨ï¼ˆä»…é™æŒ‡å®šçš„äººç±»è¯„ä¼°é˜Ÿä¼ï¼‰
    teams = get_team_list()
    if not teams:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•å›¢é˜Ÿæ•°æ®")
        exit(1)

    print(f"\nğŸ“‹ å°†å¯¹ä»¥ä¸‹ {len(teams)} ä¸ªå›¢é˜Ÿè¿›è¡Œè¯„ä¼°:")
    for i, team in enumerate(teams, 1):
        print(f"   {i}. {team}")

    # è®©ç”¨æˆ·é€‰æ‹©è¯„ä¼°æ–¹æ³•
    print("\nè¯·é€‰æ‹©è¯„ä¼°æ–¹æ³•:")
    print("1. Zero-shot è¯„ä¼° (ä¸æä¾›ç¤ºä¾‹)")
    print("2. Few-shot è¯„ä¼° (æä¾›ç¤ºä¾‹)")

    while True:
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
        if choice == "1":
            evaluation_method = "zero_shot"
            evaluation_func = etd_zero_shot_evaluation_zh
            break
        elif choice == "2":
            evaluation_method = "few_shot"
            evaluation_func = etd_few_shot_evaluation_zh
            break
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2")

    print(f"\nå·²é€‰æ‹© {evaluation_method} è¯„ä¼°æ–¹æ³•")

    # å¤„ç†æ¯ä¸ªé€‰ä¸­çš„æ¨¡å‹
    for model_idx, model_name in enumerate(selected_models, 1):
        current_model_name = model_name
        current_model_config = config[model_name]

        print(f"\nğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name} ({current_model_config['model']})")
        print(f"æ¨¡å‹è¿›åº¦: {model_idx}/{len(selected_models)}")

        print(f"\nğŸ“Š è¯„ä¼°è®¾ç½®:")
        print(f"   - è¯„ä¼°æ–¹æ³•: {evaluation_method}")
        print(f"   - APIæ¨¡å‹: {current_model_config['model']}")
        print(f"   - æ¸©åº¦å‚æ•°: {current_model_config['temperature']}")

        # å¼€å§‹è¯„ä¼°æ‰€æœ‰å›¢é˜Ÿ
        print(f"\nğŸ¯ å¼€å§‹æ‰¹é‡è¯„ä¼°...")

        completed_teams = []
        failed_teams = []

        for i, team_name in enumerate(teams, 1):
            try:
                print(f"\n{'='*60}")
                print(f"ğŸ“Š è¿›åº¦: {i}/{len(teams)} - è¯„ä¼°å›¢é˜Ÿ: {team_name} (æ¨¡å‹: {model_name})")
                print(f"{'='*60}")

                # é’ˆå¯¹è¯¥é˜ŸåŠ è½½å…¶ Chinese.tsv
                chinese_df = load_chinese_test_data_for_team(team_name)
                if chinese_df is None or chinese_df.empty:
                    print(f"âš ï¸  è·³è¿‡ {team_name}ï¼ˆæ— ä¸­æ–‡æ•°æ®ï¼‰")
                    continue

                result_file = process_team_evaluation(team_name, chinese_df, evaluation_func, evaluation_method)
                completed_teams.append((team_name, result_file))

            except KeyboardInterrupt:
                print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢è¯„ä¼°")
                break
            except Exception as e:
                print(f"âŒ å›¢é˜Ÿ {team_name} è¯„ä¼°å¤±è´¥: {e}")
                failed_teams.append((team_name, str(e)))
                continue

        # è¾“å‡ºå½“å‰æ¨¡å‹çš„ç»Ÿè®¡
        print(f"\nğŸ‰ æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆ!")
        print(f"âœ… æˆåŠŸå®Œæˆ: {len(completed_teams)} ä¸ªå›¢é˜Ÿ")
        print(f"âŒ è¯„ä¼°å¤±è´¥: {len(failed_teams)} ä¸ªå›¢é˜Ÿ")

        if completed_teams:
            print(f"\nğŸ“ æˆåŠŸå®Œæˆçš„å›¢é˜Ÿ:")
            for team, file_path in completed_teams:
                print(f"   - {team}: {file_path}")

        if failed_teams:
            print(f"\nâš ï¸  å¤±è´¥çš„å›¢é˜Ÿ:")
            for team, error in failed_teams:
                print(f"   - {team}: {error}")

    print(f"\nğŸ‰ æ‰€æœ‰æ¨¡å‹å’Œå›¢é˜Ÿå¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {os.path.join(os.path.dirname(script_dir), 'llm_evolution')} (å„é˜Ÿä¼çš„ *_zh ç›®å½•ä¸­)")


if __name__ == "__main__":
    main()
