#!/usr/bin/env python3
"""
å¯¹æ¯”äººå·¥è¯„ä»·ã€LLMè¯„ä»·å’Œæœºå™¨è¯„ä»·çš„ç»“æœ
åŒ…å«å¤šç§ä¸€è‡´æ€§è¯„ä»·æŒ‡æ ‡å’Œè¯„ä»·è´¨é‡åˆ†æ
"""
import pandas as pd
import numpy as np
import os
from scipy.stats import kendalltau, spearmanr, pearsonr, rankdata
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings

warnings.filterwarnings("ignore")


def collect_new_model_results(evaluation_type):
    """æ”¶é›†æ–°æ¨¡å‹çš„è¯„ä»·ç»“æœ"""
    import glob

    base_dir = "data/result/llm_evolution"
    if not os.path.exists(base_dir):
        return {}

    results = {}
    models = ["deepseek", "qwen"]

    for model in models:
        model_results = []
        team_dirs = glob.glob(os.path.join(base_dir, "*_en"))

        for team_dir in team_dirs:
            team_name = os.path.basename(team_dir).replace("_en", "")
            model_dir = os.path.join(team_dir, model)

            if os.path.exists(model_dir):
                if evaluation_type == "zero_shot":
                    result_file = os.path.join(model_dir, f"etd_zero_shot_results_en_ENPrompt_{model}.csv")
                else:
                    result_file = os.path.join(model_dir, f"etd_few_shot_results_en_ENPrompt_{model}.csv")

                if os.path.exists(result_file):
                    try:
                        df = pd.read_csv(result_file)
                        if len(df) > 0:
                            stats_data = {
                                "team_name": team_name,
                                "j_score_mean": df["j_score"].mean(),
                                "sta_mean": df["STA"].mean(),
                                "cs_mean": df["CS"].mean(),
                                "fs_mean": df["FS"].mean(),
                                "total_samples": len(df)
                            }
                            model_results.append(stats_data)
                    except Exception as e:
                        print(f"âŒ è¯»å– {result_file} å¤±è´¥: {e}")

        if model_results:
            results[model] = pd.DataFrame(model_results)

    return results


def create_ranking_from_new_model(model_name, evaluation_type):
    """ä»æ–°æ¨¡å‹ç»“æœåˆ›å»ºæ’åDataFrame"""
    new_results = collect_new_model_results(evaluation_type)

    if model_name not in new_results:
        print(f"âŒ æœªæ‰¾åˆ° {model_name} æ¨¡å‹çš„ç»“æœ")
        return pd.DataFrame()

    df = new_results[model_name].copy()
    df = df.sort_values("j_score_mean", ascending=False).reset_index(drop=True)
    print(f"âœ… ä½¿ç”¨ {model_name} æ¨¡å‹ç»“æœ: {len(df)} ä¸ªå›¢é˜Ÿ")

    return df


def compare_all_new_models(evaluation_type, human_df, machine_df):
    """å¯¹æ¯”æ‰€æœ‰æ–°æ¨¡å‹çš„ç»“æœ"""
    new_results = collect_new_model_results(evaluation_type)

    if not new_results:
        print("âŒ æœªæ‰¾åˆ°æ–°æ¨¡å‹ç»“æœ")
        return

    print(f"\nğŸ¤– å¯¹æ¯”æ‰€æœ‰æ–°æ¨¡å‹ ({evaluation_type.replace('_', '-').title()} è¯„ä¼°)")
    print("=" * 80)

    for model_name, model_df in new_results.items():
        print(f"\nğŸ“ˆ {model_name.upper()} æ¨¡å‹ç»“æœ:")
        print("-" * 50)

        # é‡æ–°æ’åºå¹¶æ·»åŠ æ’å
        model_df = model_df.sort_values("j_score_mean", ascending=False).reset_index(drop=True)
        model_df["llm_rank"] = model_df.index + 1

        # æ˜¾ç¤ºå‰10å
        print(f"{'æ’å':<4} {'å›¢é˜Ÿåç§°':<25} {'J-Score':<8} {'STA':<6} {'CS':<6} {'FS':<6}")
        print("-" * 60)
        for _, row in model_df.head(10).iterrows():
            print(f"{row['llm_rank']:<4} {row['team_name']:<25} {row['j_score_mean']:<8.3f} "
                  f"{row['sta_mean']:<6.2f} {row['cs_mean']:<6.2f} {row['fs_mean']:<6.2f}")

        # ä¸äººå·¥è¯„ä»·å¯¹æ¯”
        analyze_model_correlation(model_df, human_df, model_name, evaluation_type)


def analyze_model_correlation(model_df, human_df, model_name, evaluation_type):
    """åˆ†ææ¨¡å‹ä¸äººå·¥è¯„ä»·çš„ç›¸å…³æ€§"""
    from scipy.stats import spearmanr, pearsonr, kendalltau

    # å›¢é˜Ÿåç§°æ˜ å°„
    name_mapping = {
        "Team SINAI": "Team_SINAI",
        "delete": "delete_baseline",
        "mT5": "mt5_baseline",
        "backtranslation": "backtranslation_baseline",
        "Team MarSanAI": "Team MarSan_AI",
        "Team nlpjoyers": "Team nlp_enjoyers",
    }

    # åˆå¹¶æ•°æ®è¿›è¡Œå¯¹æ¯”
    comparison_data = []
    for _, model_row in model_df.iterrows():
        team = model_row["team_name"]

        # æŸ¥æ‰¾å¯¹åº”çš„äººå·¥è¯„ä»·
        human_row = human_df[human_df["team_normalized"] == team]

        if len(human_row) > 0:
            comparison_data.append({
                "team_name": team,
                "human_score": human_row.iloc[0]["human_score"],
                "human_rank": human_row.iloc[0]["human_rank"],
                "llm_score": model_row["j_score_mean"],
                "llm_rank": model_row["llm_rank"],
            })

    if len(comparison_data) > 2:
        comparison_df = pd.DataFrame(comparison_data)

        # è®¡ç®—ç›¸å…³æ€§
        human_scores = comparison_df["human_score"].values
        llm_scores = comparison_df["llm_score"].values

        spearman_corr, spearman_p = spearmanr(human_scores, llm_scores)
        pearson_corr, pearson_p = pearsonr(human_scores, llm_scores)
        kendall_corr, kendall_p = kendalltau(human_scores, llm_scores)

        print(f"\nğŸ”— {model_name.upper()} ä¸äººå·¥è¯„ä»·çš„ç›¸å…³æ€§åˆ†æ:")
        print(f"   â€¢ åŸºäº {len(comparison_df)} ä¸ªå›¢é˜Ÿçš„å¯¹æ¯”:")
        print(f"   â€¢ Spearmanç›¸å…³ç³»æ•°: {spearman_corr:.3f} (p={spearman_p:.3f})")
        print(f"   â€¢ Pearsonç›¸å…³ç³»æ•°:  {pearson_corr:.3f} (p={pearson_p:.3f})")
        print(f"   â€¢ Kendall's tau:   {kendall_corr:.3f} (p={kendall_p:.3f})")

        # æ’åå·®å¼‚åˆ†æ
        comparison_df["rank_diff"] = comparison_df["llm_rank"] - comparison_df["human_rank"]
        mean_rank_diff = comparison_df["rank_diff"].mean()
        abs_mean_rank_diff = comparison_df["rank_diff"].abs().mean()

        print(f"   â€¢ å¹³å‡æ’åå·®å¼‚: {mean_rank_diff:+.1f} ä½")
        print(f"   â€¢ ç»å¯¹æ’åå·®å¼‚: {abs_mean_rank_diff:.1f} ä½")

        # ä¿å­˜ç»“æœ
        output_dir = os.path.join("data", "evaluation_results", "new_models_comparison")
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"{model_name}_{evaluation_type}_detailed_comparison.csv")
        comparison_df.to_csv(output_file, index=False)
        print(f"   ğŸ“ è¯¦ç»†å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    else:
        print(f"   âŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç›¸å…³æ€§åˆ†æ (åªæœ‰ {len(comparison_data)} ä¸ªåŒ¹é…å›¢é˜Ÿ)")


def compare_all_evaluations():
    """å¯¹æ¯”ä¸‰ç§è¯„ä»·æ–¹æ³•çš„ç»“æœ"""

    # äººå·¥è¯„ä»·æ•°æ®ï¼ˆè‹±æ–‡åˆ†æ•°ï¼‰
    human_data = [
        {"team": "SomethingAwful", "human_score": 0.86},
        {"team": "VitalyProtasov", "human_score": 0.69},
        {"team": "nikita.sushko", "human_score": 0.70},
        {"team": "erehulka", "human_score": 0.88},
        {"team": "Team NLPunks", "human_score": 0.84},
        {"team": "mkrisnai", "human_score": 0.89},
        {"team": "Team cake", "human_score": 0.91},
        {"team": "Team SINAI", "human_score": 0.85},
        {"team": "gleb.shnshn", "human_score": 0.74},
        {"team": "delete", "human_score": 0.47},
        {"team": "mT5", "human_score": 0.68},
        {"team": "Team nlp_enjoyers", "human_score": 0.67},
        {"team": "Team Iron Autobots", "human_score": 0.74},
        {"team": "ZhongyuLuo", "human_score": 0.73},
        {"team": "backtranslation", "human_score": 0.73},
        {"team": "Team MarSanAI", "human_score": 0.89},
        {"team": "dkenco", "human_score": 0.68},
    ]

    # æœºå™¨è¯„ä»·æ•°æ®ï¼ˆä»å›¾ç‰‡ä¸­æå–ï¼Œè‹±æ–‡åˆ—ï¼‰
    machine_data = [
        {"team": "Team SmurfCat", "machine_score": 0.602},
        {"team": "Imeribal", "machine_score": 0.593},
        {"team": "nikita.sushko", "machine_score": 0.553},
        {"team": "VitalyProtasov", "machine_score": 0.531},
        {"team": "erehulka", "machine_score": 0.543},
        {"team": "SomethingAwful", "machine_score": 0.522},
        {"team": "mareksuppa", "machine_score": 0.537},
        {"team": "kofeinix", "machine_score": 0.497},
        {"team": "Yekaterina29", "machine_score": 0.510},
        {"team": "AlekseevArtem", "machine_score": 0.427},
        {"team": "Team NLPunks", "machine_score": 0.489},
        {"team": "pavelshtykov", "machine_score": 0.489},
        {"team": "gleb.shnshn", "machine_score": 0.462},
        {"team": "Volodimirich", "machine_score": 0.472},
        {"team": "ansafronov", "machine_score": 0.506},
        {"team": "MOOsipenko", "machine_score": 0.411},
        {"team": "mkrisnai", "machine_score": 0.475},
        {"team": "Team MarSanAI", "machine_score": 0.504},
        {"team": "Team nlp_enjoyers", "machine_score": 0.418},
        {"team": "Team cake", "machine_score": 0.468},
        {"team": "mT5", "machine_score": 0.418},
        {"team": "gangopsa", "machine_score": 0.472},
        {"team": "Team SINAI", "machine_score": 0.413},
        {"team": "delete", "machine_score": 0.447},
        {"team": "Team Iron Autobots", "machine_score": 0.345},
        {"team": "LanaKlitotekhnis", "machine_score": 0.460},
        {"team": "Anastasia1706", "machine_score": 0.349},
        {"team": "ZhongyuLuo", "machine_score": 0.506},
        {"team": "cocount", "machine_score": 0.271},
        {"team": "backtranslation", "machine_score": 0.506},
        {"team": "etomoscow", "machine_score": 0.293},
        {"team": "cointegrated", "machine_score": 0.160},
        {"team": "dkenco", "machine_score": 0.183},
        {"team": "FD", "machine_score": 0.061},
        {"team": "duplicate", "machine_score": 0.061},
    ]

    # åˆ›å»ºDataFrame
    human_df = pd.DataFrame(human_data)
    machine_df = pd.DataFrame(machine_data)

    # å›¢é˜Ÿåç§°æ˜ å°„
    name_mapping = {
        "Team SINAI": "Team_SINAI",
        "delete": "delete_baseline",
        "mT5": "mt5_baseline",
        "backtranslation": "backtranslation_baseline",
        "Team MarSanAI": "Team MarSan_AI",
    }

    # LLMè¯„ä»·æ–‡ä»¶ä¸­çš„å›¢é˜Ÿåç§°æ˜ å°„ï¼ˆå¤„ç†åç§°ä¸ä¸€è‡´é—®é¢˜ï¼‰
    llm_name_mapping = {"Team nlpjoyers": "Team nlp_enjoyers"}  # LLMæ–‡ä»¶ä¸­ç¼ºå°‘ä¸‹åˆ’çº¿

    # åº”ç”¨åç§°æ˜ å°„
    human_df["team_normalized"] = (
        human_df["team"].map(name_mapping).fillna(human_df["team"])
    )
    machine_df["team_normalized"] = (
        machine_df["team"].map(name_mapping).fillna(machine_df["team"])
    )

    # è®¡ç®—æ’å
    human_df = human_df.sort_values("human_score", ascending=False).reset_index(
        drop=True
    )
    human_df["human_rank"] = human_df.index + 1

    machine_df = machine_df.sort_values("machine_score", ascending=False).reset_index(
        drop=True
    )
    machine_df["machine_rank"] = machine_df.index + 1

    # æ£€æŸ¥å¯ç”¨çš„æ¨¡å‹ç»“æœ
    llm_candidates = [
        "llm_evaluation_ranking_zero_shot.csv",
        os.path.join(
            "data",
            "ranking_results",
            "en",
            "llm_evaluation_ranking_zero_shot.csv",
        ),
        os.path.join(
            "data",
            "result",
            "llm_evaluation",
            "EN",
            "llm_evaluation_ranking_zero_shot.csv",
        ),
    ]

    llm_file = next((p for p in llm_candidates if os.path.exists(p)), None)
    new_model_results = collect_new_model_results("zero_shot")

    # è®©ç”¨æˆ·é€‰æ‹©ä½¿ç”¨å“ªç§æ¨¡å‹ç»“æœ
    print("\nğŸ¤– é€‰æ‹©è¦åˆ†æçš„æ¨¡å‹ç»“æœ:")
    options = []

    if llm_file:
        options.append(("OpenAIæ¨¡å‹ (Zero-Shot)", "openai"))
        print(f"1. OpenAIæ¨¡å‹ (Zero-Shot)")

    if new_model_results:
        if "deepseek" in new_model_results:
            options.append(("DeepSeekæ¨¡å‹ (Zero-Shot)", "deepseek"))
            print(f"{len(options)+1}. DeepSeekæ¨¡å‹ (Zero-Shot)")

        if "qwen" in new_model_results:
            options.append(("Qwenæ¨¡å‹ (Zero-Shot)", "qwen"))
            print(f"{len(options)+1}. Qwenæ¨¡å‹ (Zero-Shot)")

        options.append(("å¯¹æ¯”æ‰€æœ‰æ–°æ¨¡å‹", "compare_all"))
        print(f"{len(options)+1}. å¯¹æ¯”æ‰€æœ‰æ–°æ¨¡å‹")

    if not options:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•LLMè¯„ä»·ç»“æœæ–‡ä»¶ã€‚è¯·ç¡®è®¤æ˜¯å¦å·²ç”Ÿæˆæ¨¡å‹ç»“æœã€‚")
        return

    while True:
        try:
            choice = int(input(f"\nè¯·é€‰æ‹© (1-{len(options)}): ").strip())
            if 1 <= choice <= len(options):
                selected_option = options[choice-1][1]
                break
            else:
                print(f"è¯·è¾“å…¥ 1-{len(options)} ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")

    if selected_option == "openai":
        llm_df = pd.read_csv(llm_file)
        print(f"âœ… ä½¿ç”¨OpenAIæ¨¡å‹ç»“æœ: {len(llm_df)} ä¸ªå›¢é˜Ÿ")
    elif selected_option == "deepseek":
        llm_df = create_ranking_from_new_model("deepseek", "zero_shot")
    elif selected_option == "qwen":
        llm_df = create_ranking_from_new_model("qwen", "zero_shot")
    elif selected_option == "compare_all":
        compare_all_new_models("zero_shot", human_df, machine_df)
        return

    if llm_df.empty:
        print("âŒ æ— æ³•åŠ è½½é€‰æ‹©çš„æ¨¡å‹ç»“æœ")
        return

    # åº”ç”¨LLMå›¢é˜Ÿåç§°æ˜ å°„
    llm_df["team_name"] = (
        llm_df["team_name"].map(llm_name_mapping).fillna(llm_df["team_name"])
    )

    llm_df = llm_df.sort_values("j_score_mean", ascending=False).reset_index(drop=True)
    llm_df["llm_rank"] = llm_df.index + 1

    print("ğŸ” ä¸‰ç§è¯„ä»·æ–¹æ³•å¯¹æ¯”åˆ†æï¼ˆä»¥äººå·¥è¯„ä»·ä¸ºå‡†ï¼‰")
    print("=" * 100)

    # ä»¥äººå·¥è¯„ä»·ä¸ºå‡†ï¼Œæ‰¾å‡ºæœ‰äººå·¥è¯„ä»·çš„å›¢é˜Ÿ
    human_teams = set(human_df["team_normalized"])

    print(f"äººå·¥è¯„ä»·å›¢é˜Ÿæ•°: {len(human_teams)} ä¸ª")

    # è¿‡æ»¤æœºå™¨è¯„ä»·å’ŒLLMè¯„ä»·ï¼Œåªä¿ç•™æœ‰äººå·¥è¯„ä»·çš„å›¢é˜Ÿ
    machine_df_filtered = machine_df[
        machine_df["team_normalized"].isin(human_teams)
    ].copy()
    llm_df_filtered = llm_df[llm_df["team_name"].isin(human_teams)].copy()

    # é‡æ–°è®¡ç®—æœºå™¨è¯„ä»·å’ŒLLMè¯„ä»·çš„æ’åï¼ˆåªé’ˆå¯¹æœ‰äººå·¥è¯„ä»·çš„å›¢é˜Ÿï¼‰
    machine_df_filtered = machine_df_filtered.sort_values(
        "machine_score", ascending=False
    ).reset_index(drop=True)
    machine_df_filtered["machine_rank"] = machine_df_filtered.index + 1

    llm_df_filtered = llm_df_filtered.sort_values(
        "j_score_mean", ascending=False
    ).reset_index(drop=True)
    llm_df_filtered["llm_rank"] = llm_df_filtered.index + 1

    print(f"è¿‡æ»¤åæœºå™¨è¯„ä»·å›¢é˜Ÿæ•°: {len(machine_df_filtered)} ä¸ª")
    print(f"è¿‡æ»¤åLLMè¯„ä»·å›¢é˜Ÿæ•°: {len(llm_df_filtered)} ä¸ª")

    # åˆå¹¶æ•°æ®
    comparison_data = []

    for _, human_row in human_df.iterrows():
        team = human_row["team_normalized"]

        # æŸ¥æ‰¾å¯¹åº”çš„æœºå™¨è¯„ä»·å’ŒLLMè¯„ä»·
        machine_row = machine_df_filtered[
            machine_df_filtered["team_normalized"] == team
        ]
        llm_row = llm_df_filtered[llm_df_filtered["team_name"] == team]

        comparison_data.append(
            {
                "team_name": team,
                "human_rank": human_row["human_rank"],
                "human_score": human_row["human_score"],
                "machine_rank": (
                    machine_row.iloc[0]["machine_rank"]
                    if len(machine_row) > 0
                    else None
                ),
                "machine_score": (
                    machine_row.iloc[0]["machine_score"]
                    if len(machine_row) > 0
                    else None
                ),
                "llm_rank": llm_row.iloc[0]["llm_rank"] if len(llm_row) > 0 else None,
                "llm_score": (
                    llm_row.iloc[0]["j_score_mean"] if len(llm_row) > 0 else None
                ),
            }
        )

    comparison_df = pd.DataFrame(comparison_data)

    # æŒ‰äººå·¥æ’åæ’åºæ˜¾ç¤º
    comparison_df = comparison_df.sort_values("human_rank")

    print(f"\nğŸ“Š è¯¦ç»†å¯¹æ¯” (å…± {len(comparison_df)} ä¸ªå›¢é˜Ÿ)")
    print("-" * 100)
    print(
        f"{'å›¢é˜Ÿåç§°':<25} {'äººå·¥æ’å':<8} {'äººå·¥åˆ†æ•°':<8} {'æœºå™¨æ’å':<8} {'æœºå™¨åˆ†æ•°':<8} {'LLMæ’å':<8} {'LLMåˆ†æ•°':<8}"
    )
    print("-" * 100)

    for _, row in comparison_df.iterrows():
        machine_rank_str = (
            str(int(row["machine_rank"])) if pd.notna(row["machine_rank"]) else "N/A"
        )
        machine_score_str = (
            f"{row['machine_score']:.3f}" if pd.notna(row["machine_score"]) else "N/A"
        )
        llm_rank_str = str(int(row["llm_rank"])) if pd.notna(row["llm_rank"]) else "N/A"
        llm_score_str = (
            f"{row['llm_score']:.3f}" if pd.notna(row["llm_score"]) else "N/A"
        )

        print(
            f"{row['team_name']:<25} {row['human_rank']:<8} {row['human_score']:<8.2f} "
            f"{machine_rank_str:<8} {machine_score_str:<8} "
            f"{llm_rank_str:<8} {llm_score_str:<8}"
        )

    # è¯¦ç»†ç›¸å…³æ€§åˆ†æ
    print(f"\nğŸ”— è¯¦ç»†ç›¸å…³æ€§åˆ†æ:")
    print("=" * 80)

    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    valid_machine_df = comparison_df.dropna(subset=["machine_score"])
    valid_llm_df = comparison_df.dropna(subset=["llm_score"])
    valid_all_df = comparison_df.dropna(subset=["machine_score", "llm_score"])

    def calculate_correlations(x, y, name1, name2, n_samples):
        """è®¡ç®—å¤šç§ç›¸å…³æ€§æŒ‡æ ‡"""
        if len(x) < 2:
            print(f"   {name1} vs {name2}: æ•°æ®ä¸è¶³ (åªæœ‰ {len(x)} ä¸ªæ ·æœ¬)")
            return

        # Pearsonç›¸å…³ç³»æ•° (çº¿æ€§ç›¸å…³)
        pearson_r, pearson_p = pearsonr(x, y)

        # Spearmanç­‰çº§ç›¸å…³ç³»æ•° (å•è°ƒç›¸å…³)
        spearman_r, spearman_p = spearmanr(x, y)

        # Kendall's tau (æ’åä¸€è‡´æ€§)
        kendall_tau, kendall_p = kendalltau(x, y)

        print(f"\n   ğŸ“Š {name1} vs {name2} (åŸºäº {n_samples} ä¸ªå›¢é˜Ÿ):")
        print(f"      â€¢ Pearsonç›¸å…³ç³»æ•°:  {pearson_r:.3f} (p={pearson_p:.3f})")
        print(f"      â€¢ Spearmanç­‰çº§ç›¸å…³: {spearman_r:.3f} (p={spearman_p:.3f})")
        print(f"      â€¢ Kendall's tau:   {kendall_tau:.3f} (p={kendall_p:.3f})")

        # è§£é‡Šç›¸å…³æ€§å¼ºåº¦
        def interpret_correlation(r):
            abs_r = abs(r)
            if abs_r >= 0.9:
                return "éå¸¸å¼º"
            elif abs_r >= 0.7:
                return "å¼º"
            elif abs_r >= 0.5:
                return "ä¸­ç­‰"
            elif abs_r >= 0.3:
                return "å¼±"
            else:
                return "å¾ˆå¼±"

        print(f"      â€¢ ç›¸å…³æ€§å¼ºåº¦: {interpret_correlation(spearman_r)}")

        # è®¡ç®—é¢„æµ‹è¯¯å·®
        mae = mean_absolute_error(x, y)
        rmse = np.sqrt(mean_squared_error(x, y))
        print(f"      â€¢ å¹³å‡ç»å¯¹è¯¯å·®(MAE): {mae:.3f}")
        print(f"      â€¢ å‡æ–¹æ ¹è¯¯å·®(RMSE): {rmse:.3f}")
        
        # æ·»åŠ æ–¹å·®å’Œç»å¯¹å€¼åˆ†æ
        print(f"      â€¢ æ–¹å·®åˆ†æ:")
        x_var = np.var(x, ddof=1)
        y_var = np.var(y, ddof=1)
        x_std = np.std(x, ddof=1)
        y_std = np.std(y, ddof=1)
        print(f"        - {name1}æ–¹å·®: {x_var:.4f} (æ ‡å‡†å·®: {x_std:.3f})")
        print(f"        - {name2}æ–¹å·®: {y_var:.4f} (æ ‡å‡†å·®: {y_std:.3f})")
        
        # æ–¹å·®æ¯”è¾ƒ
        var_ratio = x_var / y_var if y_var > 0 else float('inf')
        if var_ratio > 1.5:
            print(f"        - æ–¹å·®æ¯”è¾ƒ: {name1}å˜å¼‚æ€§æ›´å¤§ (æ¯”å€¼: {var_ratio:.2f})")
        elif var_ratio < 0.67:
            print(f"        - æ–¹å·®æ¯”è¾ƒ: {name2}å˜å¼‚æ€§æ›´å¤§ (æ¯”å€¼: {var_ratio:.2f})")
        else:
            print(f"        - æ–¹å·®æ¯”è¾ƒ: å˜å¼‚æ€§ç›¸è¿‘ (æ¯”å€¼: {var_ratio:.2f})")
        
        # ç»å¯¹å€¼åˆ†æ
        print(f"      â€¢ ç»å¯¹å€¼åˆ†æ:")
        diff = np.array(x) - np.array(y)
        abs_diff = np.abs(diff)
        mean_abs_diff = np.mean(abs_diff)
        median_abs_diff = np.median(abs_diff)
        max_abs_diff = np.max(abs_diff)
        min_abs_diff = np.min(abs_diff)
        
        # æŒ‰ç…§å…¬å¼(1)å’Œ(2)è®¡ç®— abs å’Œ var æŒ‡æ ‡
        # å…¬å¼(1): abs = (1/N) * Î£|JS_MER - JS_M| ï¼ˆä¸å¹³å‡ç»å¯¹å·®å€¼ç›¸åŒï¼‰
        print(f"        - abs (å…¬å¼1): {mean_abs_diff:.3f}")
        
        # å…¬å¼(2): var = (1/N) * Î£|JS_MER - JS_M|Â²  
        var_formula = np.mean((diff) ** 2)
        print(f"        - var (å…¬å¼2): {var_formula:.3f}")
        
        print(f"        - å¹³å‡ç»å¯¹å·®å€¼: {mean_abs_diff:.3f}")
        print(f"        - ä¸­ä½æ•°ç»å¯¹å·®å€¼: {median_abs_diff:.3f}")
        print(f"        - æœ€å¤§ç»å¯¹å·®å€¼: {max_abs_diff:.3f}")
        print(f"        - æœ€å°ç»å¯¹å·®å€¼: {min_abs_diff:.3f}")
        
        # ç»å¯¹å·®å€¼åˆ†å¸ƒ
        small_diff_count = np.sum(abs_diff <= 0.1)
        medium_diff_count = np.sum((abs_diff > 0.1) & (abs_diff <= 0.2))
        large_diff_count = np.sum(abs_diff > 0.2)
        
        print(f"        - ç»å¯¹å·®å€¼åˆ†å¸ƒ:")
        print(f"          * å°å·®å¼‚(â‰¤0.1): {small_diff_count} ä¸ª ({small_diff_count/len(abs_diff)*100:.1f}%)")
        print(f"          * ä¸­ç­‰å·®å¼‚(0.1-0.2): {medium_diff_count} ä¸ª ({medium_diff_count/len(abs_diff)*100:.1f}%)")
        print(f"          * å¤§å·®å¼‚(>0.2): {large_diff_count} ä¸ª ({large_diff_count/len(abs_diff)*100:.1f}%)")
        
        # ä¸€è‡´æ€§æŒ‡æ ‡
        consistency_threshold = 0.1  # å¯è°ƒæ•´çš„ä¸€è‡´æ€§é˜ˆå€¼
        consistency_rate = np.sum(abs_diff <= consistency_threshold) / len(abs_diff)
        print(f"        - ä¸€è‡´æ€§ç‡(å·®å¼‚â‰¤{consistency_threshold}): {consistency_rate:.3f}")
        
        # å¼‚å¸¸å€¼æ£€æµ‹
        q75, q25 = np.percentile(abs_diff, [75, 25])
        iqr = q75 - q25
        outlier_threshold = q75 + 1.5 * iqr
        outliers = abs_diff > outlier_threshold
        outlier_count = np.sum(outliers)
        
        if outlier_count > 0:
            print(f"        - å¼‚å¸¸å·®å€¼: {outlier_count} ä¸ª (é˜ˆå€¼: {outlier_threshold:.3f})")
            # æ‰¾å‡ºå¼‚å¸¸å€¼å¯¹åº”çš„å›¢é˜Ÿï¼ˆå¦‚æœæœ‰å›¢é˜Ÿä¿¡æ¯çš„è¯ï¼‰
            if outlier_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªå¼‚å¸¸å€¼
                outlier_indices = np.where(outliers)[0]
                for idx in outlier_indices[:3]:
                    print(f"          * ç´¢å¼•{idx}: å·®å€¼ {abs_diff[idx]:.3f}")
        else:
            print(f"        - å¼‚å¸¸å·®å€¼: æ—  (é˜ˆå€¼: {outlier_threshold:.3f})")

    # äººå·¥ vs æœºå™¨è¯„ä»·
    if len(valid_machine_df) > 1:
        calculate_correlations(
            valid_machine_df["human_score"],
            valid_machine_df["machine_score"],
            "äººå·¥è¯„ä»·",
            "æœºå™¨è¯„ä»·",
            len(valid_machine_df),
        )

    # äººå·¥ vs LLMè¯„ä»·
    if len(valid_llm_df) > 1:
        calculate_correlations(
            valid_llm_df["human_score"],
            valid_llm_df["llm_score"],
            "äººå·¥è¯„ä»·",
            "LLMè¯„ä»·",
            len(valid_llm_df),
        )

    # æœºå™¨ vs LLMè¯„ä»·
    if len(valid_all_df) > 1:
        calculate_correlations(
            valid_all_df["machine_score"],
            valid_all_df["llm_score"],
            "æœºå™¨è¯„ä»·",
            "LLMè¯„ä»·",
            len(valid_all_df),
        )

    # å‰5åå¯¹æ¯”
    print(f"\nğŸ† å‰5åå¯¹æ¯”:")

    print("\näººå·¥è¯„ä»·å‰5å:")
    top5_human = comparison_df.nsmallest(5, "human_rank")
    for i, (_, row) in enumerate(top5_human.iterrows(), 1):
        print(f"   {i}. {row['team_name']:<25} (åˆ†æ•°: {row['human_score']:.2f})")

    print("\næœºå™¨è¯„ä»·å‰5å:")
    valid_machine_for_top5 = comparison_df.dropna(subset=["machine_rank"])
    if len(valid_machine_for_top5) >= 5:
        top5_machine = valid_machine_for_top5.nsmallest(5, "machine_rank")
        for i, (_, row) in enumerate(top5_machine.iterrows(), 1):
            print(f"   {i}. {row['team_name']:<25} (åˆ†æ•°: {row['machine_score']:.3f})")
    else:
        print(f"   æœºå™¨è¯„ä»·æ•°æ®ä¸è¶³ï¼Œåªæœ‰ {len(valid_machine_for_top5)} ä¸ªå›¢é˜Ÿ")

    print("\nLLMè¯„ä»·å‰5å:")
    valid_llm_for_top5 = comparison_df.dropna(subset=["llm_rank"])
    if len(valid_llm_for_top5) >= 5:
        top5_llm = valid_llm_for_top5.nsmallest(5, "llm_rank")
        for i, (_, row) in enumerate(top5_llm.iterrows(), 1):
            print(f"   {i}. {row['team_name']:<25} (åˆ†æ•°: {row['llm_score']:.3f})")
    else:
        print(f"   LLMè¯„ä»·æ•°æ®ä¸è¶³ï¼Œåªæœ‰ {len(valid_llm_for_top5)} ä¸ªå›¢é˜Ÿ")

    # æ’åå·®å¼‚åˆ†æï¼ˆåªå¯¹æœ‰æ•ˆæ•°æ®è¿›è¡Œåˆ†æï¼‰
    comparison_df["human_machine_diff"] = (
        comparison_df["machine_rank"] - comparison_df["human_rank"]
    )
    comparison_df["human_llm_diff"] = (
        comparison_df["llm_rank"] - comparison_df["human_rank"]
    )
    comparison_df["machine_llm_diff"] = (
        comparison_df["llm_rank"] - comparison_df["machine_rank"]
    )

    print(f"\nğŸ“ˆ æ’åå·®å¼‚ç»Ÿè®¡:")

    # äººå·¥ vs æœºå™¨å·®å¼‚
    valid_hm_diff = comparison_df["human_machine_diff"].dropna()
    if len(valid_hm_diff) > 0:
        mean_diff_hm = valid_hm_diff.mean()
        abs_mean_diff_hm = valid_hm_diff.abs().mean()
        print(
            f"   - äººå·¥ vs æœºå™¨å¹³å‡å·®å¼‚: {mean_diff_hm:+.1f} ä½ (ç»å¯¹å·®å¼‚: {abs_mean_diff_hm:.1f} ä½)"
        )
        print(
            f"     åŸºäº {len(valid_hm_diff)} ä¸ªå›¢é˜Ÿï¼Œæ ‡å‡†å·®: {valid_hm_diff.std():.1f}"
        )
    else:
        print(f"   - äººå·¥ vs æœºå™¨: æ— æœ‰æ•ˆæ•°æ®")

    # äººå·¥ vs LLMå·®å¼‚
    valid_hl_diff = comparison_df["human_llm_diff"].dropna()
    if len(valid_hl_diff) > 0:
        mean_diff_hl = valid_hl_diff.mean()
        abs_mean_diff_hl = valid_hl_diff.abs().mean()
        print(
            f"   - äººå·¥ vs LLMå¹³å‡å·®å¼‚: {mean_diff_hl:+.1f} ä½ (ç»å¯¹å·®å¼‚: {abs_mean_diff_hl:.1f} ä½)"
        )
        print(
            f"     åŸºäº {len(valid_hl_diff)} ä¸ªå›¢é˜Ÿï¼Œæ ‡å‡†å·®: {valid_hl_diff.std():.1f}"
        )
    else:
        print(f"   - äººå·¥ vs LLM: æ— æœ‰æ•ˆæ•°æ®")

    # æœºå™¨ vs LLMå·®å¼‚
    valid_ml_diff = comparison_df["machine_llm_diff"].dropna()
    if len(valid_ml_diff) > 0:
        mean_diff_ml = valid_ml_diff.mean()
        abs_mean_diff_ml = valid_ml_diff.abs().mean()
        print(
            f"   - æœºå™¨ vs LLMå¹³å‡å·®å¼‚: {mean_diff_ml:+.1f} ä½ (ç»å¯¹å·®å¼‚: {abs_mean_diff_ml:.1f} ä½)"
        )
        print(
            f"     åŸºäº {len(valid_ml_diff)} ä¸ªå›¢é˜Ÿï¼Œæ ‡å‡†å·®: {valid_ml_diff.std():.1f}"
        )
    else:
        print(f"   - æœºå™¨ vs LLM: æ— æœ‰æ•ˆæ•°æ®")

    # æ‰¾å‡ºæœ€å¤§å·®å¼‚
    print(f"\nâš ï¸  æœ€å¤§æ’åå·®å¼‚:")

    if len(valid_hm_diff) > 0:
        max_human_machine = comparison_df.loc[
            comparison_df["human_machine_diff"].abs().idxmax()
        ]
        print(
            f"   - äººå·¥ vs æœºå™¨: {max_human_machine['team_name']} ({max_human_machine['human_machine_diff']:+.0f}ä½)"
        )
    else:
        print(f"   - äººå·¥ vs æœºå™¨: æ— æœ‰æ•ˆæ•°æ®")

    if len(valid_hl_diff) > 0:
        max_human_llm = comparison_df.loc[
            comparison_df["human_llm_diff"].abs().idxmax()
        ]
        print(
            f"   - äººå·¥ vs LLM: {max_human_llm['team_name']} ({max_human_llm['human_llm_diff']:+.0f}ä½)"
        )
    else:
        print(f"   - äººå·¥ vs LLM: æ— æœ‰æ•ˆæ•°æ®")

    if len(valid_ml_diff) > 0:
        max_machine_llm = comparison_df.loc[
            comparison_df["machine_llm_diff"].abs().idxmax()
        ]
        print(
            f"   - æœºå™¨ vs LLM: {max_machine_llm['team_name']} ({max_machine_llm['machine_llm_diff']:+.0f}ä½)"
        )
    else:
        print(f"   - æœºå™¨ vs LLM: æ— æœ‰æ•ˆæ•°æ®")

    # æ’åå·®å¼‚åˆ†å¸ƒåˆ†æ
    print(f"\nğŸ“Š æ’åå·®å¼‚åˆ†å¸ƒåˆ†æ:")

    def analyze_ranking_differences(diff_series, name):
        """åˆ†ææ’åå·®å¼‚çš„åˆ†å¸ƒ"""
        if len(diff_series) == 0:
            return

        # è®¡ç®—ä¸åŒå·®å¼‚èŒƒå›´çš„å›¢é˜Ÿæ•°é‡
        exact_match = len(diff_series[diff_series == 0])
        small_diff = len(diff_series[diff_series.abs() <= 2])
        medium_diff = len(
            diff_series[(diff_series.abs() > 2) & (diff_series.abs() <= 5)]
        )
        large_diff = len(diff_series[diff_series.abs() > 5])

        total = len(diff_series)

        print(f"   {name}:")
        print(
            f"     â€¢ å®Œå…¨ä¸€è‡´ (å·®å¼‚=0): {exact_match} ä¸ª ({exact_match/total*100:.1f}%)"
        )
        print(f"     â€¢ å°å·®å¼‚ (â‰¤2ä½): {small_diff} ä¸ª ({small_diff/total*100:.1f}%)")
        print(
            f"     â€¢ ä¸­ç­‰å·®å¼‚ (3-5ä½): {medium_diff} ä¸ª ({medium_diff/total*100:.1f}%)"
        )
        print(f"     â€¢ å¤§å·®å¼‚ (>5ä½): {large_diff} ä¸ª ({large_diff/total*100:.1f}%)")

    if len(valid_hm_diff) > 0:
        analyze_ranking_differences(valid_hm_diff, "äººå·¥ vs æœºå™¨")

    if len(valid_hl_diff) > 0:
        analyze_ranking_differences(valid_hl_diff, "äººå·¥ vs LLM")

    if len(valid_ml_diff) > 0:
        analyze_ranking_differences(valid_ml_diff, "æœºå™¨ vs LLM")

    # æ’åè´¨é‡è¯„ä»·æŒ‡æ ‡
    print(f"\nğŸ“Š æ’åè´¨é‡è¯„ä»·æŒ‡æ ‡:")
    print("=" * 80)

    def calculate_ranking_quality_metrics(df):
        """è®¡ç®—æ’åè´¨é‡æŒ‡æ ‡ (NDCG, MAPç­‰)"""
        valid_all_evaluations = df.dropna(subset=["machine_rank", "llm_rank"])

        if len(valid_all_evaluations) < 3:
            print("   æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—æ’åè´¨é‡æŒ‡æ ‡")
            return

        # å°†åˆ†æ•°è½¬æ¢ä¸ºç›¸å…³æ€§ç­‰çº§ (ç”¨äºNDCGè®¡ç®—)
        def score_to_relevance(scores, levels=5):
            """å°†è¿ç»­åˆ†æ•°è½¬æ¢ä¸ºç¦»æ•£ç›¸å…³æ€§ç­‰çº§"""
            percentiles = np.linspace(0, 100, levels + 1)
            relevance = np.zeros(len(scores))
            for i, score in enumerate(scores):
                for level in range(levels, 0, -1):
                    if score >= np.percentile(scores, percentiles[level - 1]):
                        relevance[i] = level
                        break
            return relevance.astype(int)

        def calculate_dcg(relevance_scores, k=None):
            """è®¡ç®—DCG (Discounted Cumulative Gain)"""
            if k is None:
                k = len(relevance_scores)
            relevance_scores = relevance_scores[:k]
            dcg = relevance_scores[0]
            for i in range(1, len(relevance_scores)):
                dcg += relevance_scores[i] / np.log2(i + 1)
            return dcg

        def calculate_ndcg(true_relevance, predicted_ranking, k=None):
            """è®¡ç®—NDCG (Normalized Discounted Cumulative Gain)"""
            if k is None:
                k = len(true_relevance)

            # æ ¹æ®é¢„æµ‹æ’åé‡æ–°æ’åºçœŸå®ç›¸å…³æ€§
            predicted_relevance = true_relevance[predicted_ranking[:k]]

            # è®¡ç®—DCG
            dcg = calculate_dcg(predicted_relevance, k)

            # è®¡ç®—IDCG (ç†æƒ³DCG)
            ideal_relevance = np.sort(true_relevance)[::-1]
            idcg = calculate_dcg(ideal_relevance, k)

            return dcg / idcg if idcg > 0 else 0

        def calculate_map(true_relevance, predicted_ranking, k=None):
            """è®¡ç®—MAP (Mean Average Precision)"""
            if k is None:
                k = len(true_relevance)

            predicted_relevance = true_relevance[predicted_ranking[:k]]

            if np.sum(predicted_relevance > 0) == 0:
                return 0

            precision_sum = 0
            relevant_count = 0

            for i in range(len(predicted_relevance)):
                if predicted_relevance[i] > 0:
                    relevant_count += 1
                    precision_at_i = relevant_count / (i + 1)
                    precision_sum += precision_at_i

            return (
                precision_sum / np.sum(true_relevance > 0)
                if np.sum(true_relevance > 0) > 0
                else 0
            )

        # å‡†å¤‡æ•°æ®
        human_scores = valid_all_evaluations["human_score"].values
        machine_scores = valid_all_evaluations["machine_score"].values
        llm_scores = valid_all_evaluations["llm_score"].values

        # è½¬æ¢ä¸ºç›¸å…³æ€§ç­‰çº§
        human_relevance = score_to_relevance(human_scores)

        # è·å–æ’åç´¢å¼•
        human_ranking = np.argsort(-human_scores)  # é™åºæ’åˆ—
        machine_ranking = np.argsort(-machine_scores)
        llm_ranking = np.argsort(-llm_scores)

        print(f"   åŸºäº {len(valid_all_evaluations)} ä¸ªå›¢é˜Ÿçš„æ’åè´¨é‡åˆ†æ:")

        # è®¡ç®—ä¸åŒKå€¼çš„NDCGå’ŒMAP
        for k in [3, 5, 10]:
            if k <= len(valid_all_evaluations):
                print(f"\n   ğŸ“ˆ Top-{k} æ’åè´¨é‡æŒ‡æ ‡:")

                # NDCG
                ndcg_machine = calculate_ndcg(human_relevance, machine_ranking, k)
                ndcg_llm = calculate_ndcg(human_relevance, llm_ranking, k)

                print(f"      â€¢ NDCG@{k}:")
                print(f"        - æœºå™¨è¯„ä»·: {ndcg_machine:.3f}")
                print(f"        - LLMè¯„ä»·:  {ndcg_llm:.3f}")

                # MAP
                map_machine = calculate_map(human_relevance, machine_ranking, k)
                map_llm = calculate_map(human_relevance, llm_ranking, k)

                print(f"      â€¢ MAP@{k}:")
                print(f"        - æœºå™¨è¯„ä»·: {map_machine:.3f}")
                print(f"        - LLMè¯„ä»·:  {map_llm:.3f}")

                # æ¯”è¾ƒå“ªä¸ªæ–¹æ³•æ›´å¥½
                if ndcg_machine > ndcg_llm:
                    better_ndcg = "æœºå™¨è¯„ä»·"
                    ndcg_diff = ndcg_machine - ndcg_llm
                elif ndcg_llm > ndcg_machine:
                    better_ndcg = "LLMè¯„ä»·"
                    ndcg_diff = ndcg_llm - ndcg_machine
                else:
                    better_ndcg = "ç›¸ç­‰"
                    ndcg_diff = 0

                if map_machine > map_llm:
                    better_map = "æœºå™¨è¯„ä»·"
                    map_diff = map_machine - map_llm
                elif map_llm > map_machine:
                    better_map = "LLMè¯„ä»·"
                    map_diff = map_llm - map_machine
                else:
                    better_map = "ç›¸ç­‰"
                    map_diff = 0

                print(f"      â€¢ æ’åè´¨é‡æ¯”è¾ƒ:")
                print(f"        - NDCG@{k}: {better_ndcg} æ›´å¥½ (å·®å¼‚: {ndcg_diff:.3f})")
                print(f"        - MAP@{k}:  {better_map} æ›´å¥½ (å·®å¼‚: {map_diff:.3f})")

        # æ•´ä½“æ’åè´¨é‡è¯„ä¼°
        print(f"\n   ğŸ† æ•´ä½“æ’åè´¨é‡è¯„ä¼°:")

        # è®¡ç®—å®Œæ•´æ’åçš„NDCGå’ŒMAP
        full_ndcg_machine = calculate_ndcg(human_relevance, machine_ranking)
        full_ndcg_llm = calculate_ndcg(human_relevance, llm_ranking)
        full_map_machine = calculate_map(human_relevance, machine_ranking)
        full_map_llm = calculate_map(human_relevance, llm_ranking)

        print(f"      â€¢ å®Œæ•´æ’åNDCG:")
        print(f"        - æœºå™¨è¯„ä»·: {full_ndcg_machine:.3f}")
        print(f"        - LLMè¯„ä»·:  {full_ndcg_llm:.3f}")

        print(f"      â€¢ å®Œæ•´æ’åMAP:")
        print(f"        - æœºå™¨è¯„ä»·: {full_map_machine:.3f}")
        print(f"        - LLMè¯„ä»·:  {full_map_llm:.3f}")

        # ç»¼åˆè¯„ä»·
        machine_avg = (full_ndcg_machine + full_map_machine) / 2
        llm_avg = (full_ndcg_llm + full_map_llm) / 2

        print(f"\n   ğŸ¯ ç»¼åˆæ’åè´¨é‡è¯„åˆ†:")
        print(f"      â€¢ æœºå™¨è¯„ä»·ç»¼åˆå¾—åˆ†: {machine_avg:.3f}")
        print(f"      â€¢ LLMè¯„ä»·ç»¼åˆå¾—åˆ†:  {llm_avg:.3f}")

        if machine_avg > llm_avg:
            print(
                f"      â€¢ ç»“è®º: æœºå™¨è¯„ä»·åœ¨æ’åè´¨é‡ä¸Šè¡¨ç°æ›´å¥½ (ä¼˜åŠ¿: {machine_avg - llm_avg:.3f})"
            )
        elif llm_avg > machine_avg:
            print(
                f"      â€¢ ç»“è®º: LLMè¯„ä»·åœ¨æ’åè´¨é‡ä¸Šè¡¨ç°æ›´å¥½ (ä¼˜åŠ¿: {llm_avg - machine_avg:.3f})"
            )
        else:
            print(f"      â€¢ ç»“è®º: ä¸¤ç§è¯„ä»·æ–¹æ³•åœ¨æ’åè´¨é‡ä¸Šè¡¨ç°ç›¸å½“")

    calculate_ranking_quality_metrics(comparison_df)

    # ä¿å­˜ç»“æœ
    output_dir = os.path.join("data", "evaluation_results", "llm_evaluation")
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, "all_evaluations_comparison.csv")
    comparison_df.to_csv(output_file, index=False)
    print(f"\nğŸ“ å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # ç”Ÿæˆè¯„ä»·æ–¹æ³•æ¯”è¾ƒæ€»ç»“
    print(f"\nğŸ“‹ è¯„ä»·æ–¹æ³•æ¯”è¾ƒæ€»ç»“:")
    print("=" * 80)

    def generate_evaluation_summary(df):
        """ç”Ÿæˆè¯„ä»·æ–¹æ³•æ¯”è¾ƒçš„æ€»ç»“æŠ¥å‘Š"""
        valid_machine_df = df.dropna(subset=["machine_score"])
        valid_llm_df = df.dropna(subset=["llm_score"])
        valid_all_df = df.dropna(subset=["machine_score", "llm_score"])

        summary = {
            "data_coverage": {
                "total_teams": len(df),
                "machine_coverage": len(valid_machine_df),
                "llm_coverage": len(valid_llm_df),
                "complete_coverage": len(valid_all_df),
            },
            "correlations": {},
            "reliability": {},
            "ranking_quality": {},
        }

        # è®¡ç®—ç›¸å…³æ€§æŒ‡æ ‡
        if len(valid_machine_df) > 2:
            spearman_hm, _ = spearmanr(
                valid_machine_df["human_score"], valid_machine_df["machine_score"]
            )
            summary["correlations"]["human_machine"] = spearman_hm

        if len(valid_llm_df) > 2:
            spearman_hl, _ = spearmanr(
                valid_llm_df["human_score"], valid_llm_df["llm_score"]
            )
            summary["correlations"]["human_llm"] = spearman_hl

        if len(valid_all_df) > 2:
            spearman_ml, _ = spearmanr(
                valid_all_df["machine_score"], valid_all_df["llm_score"]
            )
            summary["correlations"]["machine_llm"] = spearman_ml

        print(f"   ğŸ” æ•°æ®è¦†ç›–æƒ…å†µ:")
        print(f"      â€¢ æ€»å›¢é˜Ÿæ•°: {summary['data_coverage']['total_teams']}")
        print(
            f"      â€¢ æœºå™¨è¯„ä»·è¦†ç›–: {summary['data_coverage']['machine_coverage']} ({summary['data_coverage']['machine_coverage']/summary['data_coverage']['total_teams']*100:.1f}%)"
        )
        print(
            f"      â€¢ LLMè¯„ä»·è¦†ç›–: {summary['data_coverage']['llm_coverage']} ({summary['data_coverage']['llm_coverage']/summary['data_coverage']['total_teams']*100:.1f}%)"
        )
        print(
            f"      â€¢ å®Œæ•´æ•°æ®è¦†ç›–: {summary['data_coverage']['complete_coverage']} ({summary['data_coverage']['complete_coverage']/summary['data_coverage']['total_teams']*100:.1f}%)"
        )

        print(f"\n   ğŸ“Š ç›¸å…³æ€§è¡¨ç°:")
        if "human_machine" in summary["correlations"]:
            print(
                f"      â€¢ äººå·¥ vs æœºå™¨: {summary['correlations']['human_machine']:.3f}"
            )
        if "human_llm" in summary["correlations"]:
            print(f"      â€¢ äººå·¥ vs LLM:  {summary['correlations']['human_llm']:.3f}")
        if "machine_llm" in summary["correlations"]:
            print(f"      â€¢ æœºå™¨ vs LLM:  {summary['correlations']['machine_llm']:.3f}")

        # è¯„ä»·æ–¹æ³•æ¨è
        print(f"\n   ğŸ’¡ è¯„ä»·æ–¹æ³•æ¨è:")

        machine_corr = summary["correlations"].get("human_machine", 0)
        llm_corr = summary["correlations"].get("human_llm", 0)

        if machine_corr > 0 and llm_corr > 0:
            if machine_corr > llm_corr + 0.1:
                print(f"      â€¢ æ¨èä½¿ç”¨: æœºå™¨è¯„ä»· (ä¸äººå·¥è¯„ä»·ç›¸å…³æ€§æ›´é«˜)")
                print(f"      â€¢ ä¼˜åŠ¿: å®¢è§‚æ€§å¼ºã€å¯é‡å¤æ€§å¥½ã€æˆæœ¬ä½")
                print(f"      â€¢ æ³¨æ„: å¯èƒ½æ— æ³•æ•æ‰è¯­è¨€çš„ç»†å¾®å·®åˆ«")
            elif llm_corr > machine_corr + 0.1:
                print(f"      â€¢ æ¨èä½¿ç”¨: LLMè¯„ä»· (ä¸äººå·¥è¯„ä»·ç›¸å…³æ€§æ›´é«˜)")
                print(f"      â€¢ ä¼˜åŠ¿: èƒ½ç†è§£è¯­ä¹‰ã€æ•æ‰ç»†å¾®å·®åˆ«")
                print(f"      â€¢ æ³¨æ„: å¯èƒ½å­˜åœ¨åè§ã€æˆæœ¬è¾ƒé«˜")
            else:
                print(f"      â€¢ æ¨èä½¿ç”¨: æ··åˆè¯„ä»·æ–¹æ³•")
                print(f"      â€¢ å»ºè®®: ç»“åˆæœºå™¨è¯„ä»·çš„å®¢è§‚æ€§å’ŒLLMè¯„ä»·çš„è¯­ä¹‰ç†è§£")

        print(f"\n   âš ï¸  è¯„ä»·å±€é™æ€§:")
        print(f"      â€¢ äººå·¥è¯„ä»·: ä¸»è§‚æ€§ã€æˆæœ¬é«˜ã€ä¸€è‡´æ€§å¯èƒ½ä¸è¶³")
        print(f"      â€¢ æœºå™¨è¯„ä»·: å¯èƒ½å¿½ç•¥è¯­ä¹‰ç»†èŠ‚ã€å¯¹åˆ›æ–°è¡¨è¾¾ä¸æ•æ„Ÿ")
        print(f"      â€¢ LLMè¯„ä»·: å¯èƒ½å­˜åœ¨æ¨¡å‹åè§ã€ç»“æœä¸å¤Ÿç¨³å®š")

        print(f"\n   ğŸ¯ æ”¹è¿›å»ºè®®:")
        print(f"      â€¢ å¢åŠ è¯„ä»·è€…æ•°é‡ä»¥æé«˜äººå·¥è¯„ä»·çš„å¯é æ€§")
        print(f"      â€¢ ä½¿ç”¨å¤šç§è‡ªåŠ¨è¯„ä»·æŒ‡æ ‡çš„ç»„åˆ")
        print(f"      â€¢ å®šæœŸæ ¡å‡†è‡ªåŠ¨è¯„ä»·æ–¹æ³•ä¸äººå·¥è¯„ä»·çš„ä¸€è‡´æ€§")
        print(f"      â€¢ è€ƒè™‘ä»»åŠ¡ç‰¹å®šçš„è¯„ä»·æŒ‡æ ‡")

        return summary

    generate_evaluation_summary(comparison_df)

    # é«˜çº§ä¸€è‡´æ€§åˆ†æ
    print(f"\nğŸ¯ é«˜çº§ä¸€è‡´æ€§åˆ†æ:")
    print("=" * 80)

    def calculate_ranking_agreement_metrics(df):
        """è®¡ç®—æ’åä¸€è‡´æ€§æŒ‡æ ‡"""
        valid_all_evaluations = df.dropna(subset=["machine_rank", "llm_rank"])

        if len(valid_all_evaluations) < 3:
            print("   æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæ’åä¸€è‡´æ€§åˆ†æ")
            return

        # 1. æ’åç›¸å…³æ€§ (Ranking Correlation)
        print(f"\n   ğŸ“ˆ æ’åç›¸å…³æ€§åˆ†æ (åŸºäº {len(valid_all_evaluations)} ä¸ªå›¢é˜Ÿ):")

        human_ranks = valid_all_evaluations["human_rank"].values
        machine_ranks = valid_all_evaluations["machine_rank"].values
        llm_ranks = valid_all_evaluations["llm_rank"].values

        # Spearmanæ’åç›¸å…³
        spearman_hm, _ = spearmanr(human_ranks, machine_ranks)
        spearman_hl, _ = spearmanr(human_ranks, llm_ranks)
        spearman_ml, _ = spearmanr(machine_ranks, llm_ranks)

        print(f"      â€¢ äººå·¥ vs æœºå™¨æ’åç›¸å…³: {spearman_hm:.3f}")
        print(f"      â€¢ äººå·¥ vs LLMæ’åç›¸å…³:  {spearman_hl:.3f}")
        print(f"      â€¢ æœºå™¨ vs LLMæ’åç›¸å…³:  {spearman_ml:.3f}")

        # 2. æ’åè·ç¦» (Ranking Distance)
        print(f"\n   ğŸ“ æ’åè·ç¦»åˆ†æ:")

        def kendall_distance(rank1, rank2):
            """è®¡ç®—Kendallè·ç¦» (ä¸ä¸€è‡´å¯¹çš„æ•°é‡)"""
            n = len(rank1)
            distance = 0
            for i in range(n):
                for j in range(i + 1, n):
                    if (rank1[i] - rank1[j]) * (rank2[i] - rank2[j]) < 0:
                        distance += 1
            return distance

        def normalized_kendall_distance(rank1, rank2):
            """å½’ä¸€åŒ–çš„Kendallè·ç¦»"""
            n = len(rank1)
            max_distance = n * (n - 1) // 2
            return (
                kendall_distance(rank1, rank2) / max_distance if max_distance > 0 else 0
            )

        kendall_hm = normalized_kendall_distance(human_ranks, machine_ranks)
        kendall_hl = normalized_kendall_distance(human_ranks, llm_ranks)
        kendall_ml = normalized_kendall_distance(machine_ranks, llm_ranks)

        print(
            f"      â€¢ äººå·¥ vs æœºå™¨Kendallè·ç¦»: {kendall_hm:.3f} (0=å®Œå…¨ä¸€è‡´, 1=å®Œå…¨ä¸ä¸€è‡´)"
        )
        print(f"      â€¢ äººå·¥ vs LLMKendallè·ç¦»:  {kendall_hl:.3f}")
        print(f"      â€¢ æœºå™¨ vs LLMKendallè·ç¦»:  {kendall_ml:.3f}")

        # 3. Top-Kä¸€è‡´æ€§åˆ†æ
        print(f"\n   ğŸ† Top-Kä¸€è‡´æ€§åˆ†æ:")

        def top_k_overlap(rank1, rank2, k):
            """è®¡ç®—Top-Ké‡å ç‡"""
            top_k_1 = set(np.argsort(rank1)[:k])
            top_k_2 = set(np.argsort(rank2)[:k])
            return len(top_k_1.intersection(top_k_2)) / k

        for k in [3, 5, 10]:
            if k <= len(valid_all_evaluations):
                overlap_hm = top_k_overlap(human_ranks, machine_ranks, k)
                overlap_hl = top_k_overlap(human_ranks, llm_ranks, k)
                overlap_ml = top_k_overlap(machine_ranks, llm_ranks, k)

                print(f"      â€¢ Top-{k}é‡å ç‡:")
                print(f"        - äººå·¥ vs æœºå™¨: {overlap_hm:.3f}")
                print(f"        - äººå·¥ vs LLM:  {overlap_hl:.3f}")
                print(f"        - æœºå™¨ vs LLM:  {overlap_ml:.3f}")

        # 4. ä¸€è‡´æ€§å›¢é˜Ÿè¯†åˆ«
        print(f"\n   ğŸ¯ ä¸€è‡´æ€§å›¢é˜Ÿè¯†åˆ«:")

        n_teams = len(valid_all_evaluations)

        # å‰1/3ä¸€è‡´æ€§
        top_third = n_teams // 3
        consistent_top_third = set()

        for _, row in valid_all_evaluations.iterrows():
            if (
                row["human_rank"] <= top_third
                and row["machine_rank"] <= top_third
                and row["llm_rank"] <= top_third
            ):
                consistent_top_third.add(row["team_name"])

        print(f"      â€¢ ä¸‰ç§è¯„ä»·éƒ½åœ¨å‰1/3çš„å›¢é˜Ÿ: {len(consistent_top_third)} ä¸ª")
        if consistent_top_third:
            for team in sorted(consistent_top_third):
                print(f"        * {team}")

        # å‰50%ä¸€è‡´æ€§
        top_half = n_teams // 2
        consistent_top_half = set()

        for _, row in valid_all_evaluations.iterrows():
            if (
                row["human_rank"] <= top_half
                and row["machine_rank"] <= top_half
                and row["llm_rank"] <= top_half
            ):
                consistent_top_half.add(row["team_name"])

        print(f"      â€¢ ä¸‰ç§è¯„ä»·éƒ½åœ¨å‰50%çš„å›¢é˜Ÿ: {len(consistent_top_half)} ä¸ª")
        if consistent_top_half:
            for team in sorted(consistent_top_half):
                if team not in consistent_top_third:  # åªæ˜¾ç¤ºä¸åœ¨å‰1/3çš„
                    print(f"        * {team}")

        # 5. è¯„ä»·æ–¹æ³•å¯é æ€§åˆ†æ
        print(f"\n   ğŸ” è¯„ä»·æ–¹æ³•å¯é æ€§åˆ†æ:")

        # ä»¥äººå·¥è¯„ä»·ä¸ºé‡‘æ ‡å‡†ï¼Œåˆ†æå…¶ä»–æ–¹æ³•çš„å¯é æ€§
        def calculate_reliability_metrics(human_scores, auto_scores, method_name):
            """è®¡ç®—è‡ªåŠ¨è¯„ä»·æ–¹æ³•çš„å¯é æ€§æŒ‡æ ‡"""
            if len(human_scores) < 3:
                return

            # ç›¸å…³æ€§
            corr, p_val = spearmanr(human_scores, auto_scores)

            # å¹³å‡ç»å¯¹æ’åå·®å¼‚
            human_ranks_norm = rankdata(human_scores, method="ordinal")
            auto_ranks_norm = rankdata(auto_scores, method="ordinal")
            mean_rank_diff = np.mean(np.abs(human_ranks_norm - auto_ranks_norm))

            # ä¸€è‡´æ€§æ¯”ä¾‹ (æ’åå·®å¼‚åœ¨Â±2ä»¥å†…)
            consistency_ratio = np.mean(np.abs(human_ranks_norm - auto_ranks_norm) <= 2)

            print(f"      â€¢ {method_name}å¯é æ€§:")
            print(f"        - ä¸äººå·¥è¯„ä»·ç›¸å…³æ€§: {corr:.3f} (p={p_val:.3f})")
            print(f"        - å¹³å‡æ’åå·®å¼‚: {mean_rank_diff:.1f} ä½")
            print(f"        - ä¸€è‡´æ€§æ¯”ä¾‹(Â±2ä½): {consistency_ratio:.3f}")

            # å¯é æ€§ç­‰çº§
            if corr >= 0.8:
                reliability = "é«˜"
            elif corr >= 0.6:
                reliability = "ä¸­ç­‰"
            elif corr >= 0.4:
                reliability = "è¾ƒä½"
            else:
                reliability = "ä½"
            print(f"        - å¯é æ€§ç­‰çº§: {reliability}")

        # åˆ†ææœºå™¨è¯„ä»·å¯é æ€§
        machine_data = valid_all_evaluations.dropna(subset=["machine_score"])
        if len(machine_data) > 2:
            calculate_reliability_metrics(
                machine_data["human_score"].values,
                machine_data["machine_score"].values,
                "æœºå™¨è¯„ä»·",
            )

        # åˆ†æLLMè¯„ä»·å¯é æ€§
        llm_data = valid_all_evaluations.dropna(subset=["llm_score"])
        if len(llm_data) > 2:
            calculate_reliability_metrics(
                llm_data["human_score"].values, llm_data["llm_score"].values, "LLMè¯„ä»·"
            )

    calculate_ranking_agreement_metrics(comparison_df)

    # æ˜¾ç¤ºæ•°æ®å®Œæ•´æ€§ç»Ÿè®¡
    print(f"\nğŸ“Š æ•°æ®å®Œæ•´æ€§ç»Ÿè®¡:")
    print("=" * 50)
    print(f"   - æ€»å›¢é˜Ÿæ•° (äººå·¥è¯„ä»·): {len(comparison_df)}")
    print(
        f"   - æœ‰æœºå™¨è¯„ä»·çš„å›¢é˜Ÿ: {len(comparison_df.dropna(subset=['machine_rank']))}"
    )
    print(f"   - æœ‰LLMè¯„ä»·çš„å›¢é˜Ÿ: {len(comparison_df.dropna(subset=['llm_rank']))}")
    print(
        f"   - ä¸‰ç§è¯„ä»·éƒ½æœ‰çš„å›¢é˜Ÿ: {len(comparison_df.dropna(subset=['machine_rank', 'llm_rank']))}"
    )


if __name__ == "__main__":
    compare_all_evaluations()
