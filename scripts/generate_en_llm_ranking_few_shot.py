#!/usr/bin/env python3
"""
ç”Ÿæˆè‹±æ–‡LLMè¯„ä»·æ’åæ–‡ä»¶ (Few-Shot)
åŸºäºå„é˜Ÿä¼çš„è‹±æ–‡LLM Few-Shotè¯„ä»·ç»“æœç”Ÿæˆæ±‡æ€»æ’å
"""
import pandas as pd
import numpy as np
import os
import glob

def generate_en_llm_ranking_few_shot():
    """ç”Ÿæˆè‹±æ–‡LLM Few-Shotè¯„ä»·æ’åæ–‡ä»¶"""
    
    print("ğŸš€ å¼€å§‹ç”Ÿæˆè‹±æ–‡LLM Few-Shotè¯„ä»·æ’å")
    print("=" * 60)
    
    # è‹±æ–‡LLMç»“æœç›®å½•
    llm_evolution_dir = "data/result/llm_evolution"
    
    if not os.path.exists(llm_evolution_dir):
        print(f"âŒ æœªæ‰¾åˆ°LLMè¯„ä»·ç»“æœç›®å½•: {llm_evolution_dir}")
        return
    
    # æŸ¥æ‰¾æ‰€æœ‰è‹±æ–‡few-shotç»“æœæ–‡ä»¶
    en_pattern = os.path.join(llm_evolution_dir, "*_en", "etd_few_shot_results_en_ENPrompt.csv")
    en_files = glob.glob(en_pattern)
    
    print(f"âœ… æ‰¾åˆ° {len(en_files)} ä¸ªè‹±æ–‡LLM Few-Shotè¯„ä»·ç»“æœæ–‡ä»¶")
    
    ranking_data = []
    
    for file_path in en_files:
        try:
            # ä»è·¯å¾„ä¸­æå–é˜Ÿä¼åç§°
            team_dir = os.path.dirname(file_path)
            team_name = os.path.basename(team_dir).replace("_en", "")
            
            # è¯»å–è¯„ä»·ç»“æœ
            df = pd.read_csv(file_path)
            
            if df.empty:
                print(f"âš ï¸  {team_name}: ç»“æœæ–‡ä»¶ä¸ºç©º")
                continue
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            total_samples = len(df)
            sta_mean = df['STA'].mean()
            cs_mean = df['CS'].mean()
            fs_mean = df['FS'].mean()
            j_score_mean = df['j_score'].mean()
            
            sta_std = df['STA'].std()
            cs_std = df['CS'].std()
            fs_std = df['FS'].std()
            j_score_std = df['j_score'].std()
            
            # è®¡ç®—å®Œç¾åˆ†æ•°å’Œé›¶åˆ†æ•°çš„æ•°é‡
            perfect_scores = len(df[df['j_score'] == 1.0])
            zero_scores = len(df[df['j_score'] == 0.0])
            
            ranking_data.append({
                'team_name': team_name,
                'evaluation_type': 'few_shot',
                'total_samples': total_samples,
                'sta_mean': sta_mean,
                'cs_mean': cs_mean,
                'fs_mean': fs_mean,
                'j_score_mean': j_score_mean,
                'sta_std': sta_std,
                'cs_std': cs_std,
                'fs_std': fs_std,
                'j_score_std': j_score_std,
                'perfect_scores': perfect_scores,
                'zero_scores': zero_scores,
                'file_path': f"LLMEvolution/{team_name}_en/etd_few_shot_results_en_ENPrompt.csv"
            })
            
            print(f"âœ… {team_name}: {total_samples} æ¡æ•°æ®, J-Scoreå‡å€¼: {j_score_mean:.3f}")
            
        except Exception as e:
            print(f"âŒ å¤„ç† {file_path} å¤±è´¥: {e}")
            continue
    
    if not ranking_data:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¯„ä»·æ•°æ®")
        return
    
    # åˆ›å»ºDataFrameå¹¶æŒ‰J-Scoreæ’åº
    ranking_df = pd.DataFrame(ranking_data)
    ranking_df = ranking_df.sort_values('j_score_mean', ascending=False).reset_index(drop=True)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = os.path.join("data", "ranking_results", "en")
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ’åæ–‡ä»¶
    output_file = os.path.join(output_dir, "llm_evaluation_ranking_few_shot.csv")
    ranking_df.to_csv(output_file, index=False)
    
    print(f"\nğŸ“Š Few-Shotæ’åç»“æœ (æŒ‰J-Scoreé™åº):")
    print("-" * 80)
    print(f"{'æ’å':<4} {'é˜Ÿä¼åç§°':<25} {'J-Score':<8} {'æ ·æœ¬æ•°':<6} {'å®Œç¾åˆ†':<6} {'é›¶åˆ†':<6}")
    print("-" * 80)
    
    for i, (_, row) in enumerate(ranking_df.iterrows(), 1):
        print(f"{i:<4} {row['team_name']:<25} {row['j_score_mean']:<8.3f} {row['total_samples']:<6} {row['perfect_scores']:<6} {row['zero_scores']:<6}")
    
    print(f"\nğŸ“ Few-Shotæ’åæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_file}")
    print(f"âœ… å…±å¤„ç† {len(ranking_df)} ä¸ªé˜Ÿä¼çš„è‹±æ–‡LLM Few-Shotè¯„ä»·ç»“æœ")

if __name__ == "__main__":
    generate_en_llm_ranking_few_shot()