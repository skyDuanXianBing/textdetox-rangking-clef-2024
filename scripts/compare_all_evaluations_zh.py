#!/usr/bin/env python3
"""
å¯¹æ¯”ä¸­æ–‡äººå·¥è¯„ä»·ã€LLMè¯„ä»·å’Œæœºå™¨è¯„ä»·çš„ç»“æœ
åŒ…å«å¤šç§ä¸€è‡´æ€§è¯„ä»·æŒ‡æ ‡å’Œè¯„ä»·è´¨é‡åˆ†æ
"""
import pandas as pd
import numpy as np
import os
from scipy import stats
from scipy.stats import kendalltau, spearmanr, pearsonr
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings

warnings.filterwarnings("ignore")


def compare_all_evaluations_zh():
    """å¯¹æ¯”ä¸‰ç§è¯„ä»·æ–¹æ³•çš„ä¸­æ–‡ç»“æœ"""

    # äººå·¥è¯„ä»·æ•°æ®ï¼ˆä¸­æ–‡åˆ†æ•°ï¼Œæ¥è‡ªâ€œTest Phase: Manual Evaluation Final Resultsâ€è¡¨çš„ zh åˆ—ï¼‰
    human_data = [
        {"team": "SomethingAwful", "human_score": 0.53},
        {"team": "VitalyProtasov", "human_score": 0.49},
        {"team": "nikita.sushko", "human_score": 0.47},
        {"team": "erehulka", "human_score": 0.68},
        {"team": "Team NLPunks", "human_score": 0.60},
        {"team": "mkrisnai", "human_score": 0.34},
        {"team": "Team cake", "human_score": 0.84},
        {"team": "Team SINAI", "human_score": 0.33},
        {"team": "gleb.shnshn", "human_score": 0.41},
        {"team": "delete", "human_score": 0.43},
        {"team": "mT5", "human_score": 0.43},
        {"team": "Team nlp_enjoyers", "human_score": 0.23},
        {"team": "Team Iron Autobots", "human_score": 0.53},
        {"team": "ZhongyuLuo", "human_score": 0.56},
        {"team": "backtranslation", "human_score": 0.34},
    ]

    # æœºå™¨è¯„ä»·æ•°æ®ï¼ˆä¸­æ–‡åˆ†æ•°ï¼Œæ¥è‡ªâ€œTest Phase: Automatic Evaluation Resultsâ€è¡¨çš„ zh åˆ—ï¼‰
    machine_data = [
        {"team": "nikita.sushko", "machine_score": 0.176},
        {"team": "VitalyProtasov", "machine_score": 0.175},
        {"team": "erehulka", "machine_score": 0.160},
        {"team": "SomethingAwful", "machine_score": 0.147},
        {"team": "mkrisnai", "machine_score": 0.109},
        {"team": "Team NLPunks", "machine_score": 0.150},
        {"team": "gleb.shnshn", "machine_score": 0.155},
        {"team": "Team cake", "machine_score": 0.086},
        {"team": "mT5", "machine_score": 0.096},
        {"team": "Team nlp_enjoyers", "machine_score": 0.104},
        {"team": "Team SINAI", "machine_score": 0.126},
        {"team": "delete", "machine_score": 0.175},
        {"team": "Team Iron Autobots", "machine_score": 0.124},
        {"team": "ZhongyuLuo", "machine_score": 0.052},
        {"team": "backtranslation", "machine_score": 0.027},
    ]

    # åˆ›å»ºDataFrame
    human_df = pd.DataFrame(human_data)
    machine_df = pd.DataFrame(machine_data)

    # å›¢é˜Ÿåç§°æ˜ å°„
    name_mapping = {
        "Team SINAI": "Team_SINAI",
        "delete": "delete_baseline",
        "mT5": "mt5_baseline",
        "backtranslation": "backtranslation_baseline",
    }

    # LLMè¯„ä»·æ–‡ä»¶ä¸­çš„å›¢é˜Ÿåç§°æ˜ å°„ï¼ˆå¤„ç†åç§°ä¸ä¸€è‡´é—®é¢˜ï¼‰
    llm_name_mapping = {"Team nlpjoyers": "Team nlp_enjoyers"}  # LLMæ–‡ä»¶ä¸­ç¼ºå°‘ä¸‹åˆ’çº¿

    # åº”ç”¨åç§°æ˜ å°„
    human_df["team_normalized"] = (
        human_df["team"].map(name_mapping).fillna(human_df["team"])
    )
    machine_df["team_normalized"] = (
        machine_df["team"].map(name_mapping).fillna(machine_df["team"])
    )

    # è®¡ç®—æ’å
    human_df = human_df.sort_values("human_score", ascending=False).reset_index(
        drop=True
    )
    human_df["human_rank"] = human_df.index + 1

    machine_df = machine_df.sort_values("machine_score", ascending=False).reset_index(
        drop=True
    )
    machine_df["machine_rank"] = machine_df.index + 1

    # è¯»å–ä¸­æ–‡LLMè¯„ä»·ç»“æœ
    llm_candidates = [
        "llm_evaluation_ranking_zero_shot.csv",
        os.path.join(
            "data",
            "ranking_results",
            "zh",
            "llm_evaluation_ranking_zero_shot.csv",
        ),
    ]
    llm_file = next((p for p in llm_candidates if os.path.exists(p)), None)
    if not llm_file:
        print(
            "âŒ æœªæ‰¾åˆ°ä¸­æ–‡LLMè¯„ä»·ç»“æœæ–‡ä»¶ã€‚è¯·ç¡®è®¤æ˜¯å¦å·²ç”Ÿæˆï¼š\n  - data/ranking_results/zh/llm_evaluation_ranking_zero_shot.csv"
        )
        return

    llm_df = pd.read_csv(llm_file)

    # åº”ç”¨LLMå›¢é˜Ÿåç§°æ˜ å°„
    llm_df["team_name"] = (
        llm_df["team_name"].map(llm_name_mapping).fillna(llm_df["team_name"])
    )

    llm_df = llm_df.sort_values("j_score_mean", ascending=False).reset_index(drop=True)
    llm_df["llm_rank"] = llm_df.index + 1

    print("ğŸ” ä¸‰ç§è¯„ä»·æ–¹æ³•å¯¹æ¯”åˆ†æï¼ˆä¸­æ–‡æ•°æ®ï¼Œä»¥äººå·¥è¯„ä»·ä¸ºå‡†ï¼‰")
    print("=" * 100)

    # ä»¥äººå·¥è¯„ä»·ä¸ºå‡†ï¼Œæ‰¾å‡ºæœ‰äººå·¥è¯„ä»·çš„å›¢é˜Ÿ
    human_teams = set(human_df["team_normalized"])

    print(f"äººå·¥è¯„ä»·å›¢é˜Ÿæ•°: {len(human_teams)} ä¸ª")

    # è¿‡æ»¤æœºå™¨è¯„ä»·å’ŒLLMè¯„ä»·ï¼Œåªä¿ç•™æœ‰äººå·¥è¯„ä»·çš„å›¢é˜Ÿ
    machine_df_filtered = machine_df[
        machine_df["team_normalized"].isin(human_teams)
    ].copy()
    llm_df_filtered = llm_df[llm_df["team_name"].isin(human_teams)].copy()

    # é‡æ–°è®¡ç®—æœºå™¨è¯„ä»·å’ŒLLMè¯„ä»·çš„æ’åï¼ˆåªé’ˆå¯¹æœ‰äººå·¥è¯„ä»·çš„å›¢é˜Ÿï¼‰
    machine_df_filtered = machine_df_filtered.sort_values(
        "machine_score", ascending=False
    ).reset_index(drop=True)
    machine_df_filtered["machine_rank"] = machine_df_filtered.index + 1

    llm_df_filtered = llm_df_filtered.sort_values(
        "j_score_mean", ascending=False
    ).reset_index(drop=True)
    llm_df_filtered["llm_rank"] = llm_df_filtered.index + 1

    print(f"è¿‡æ»¤åæœºå™¨è¯„ä»·å›¢é˜Ÿæ•°: {len(machine_df_filtered)} ä¸ª")
    print(f"è¿‡æ»¤åLLMè¯„ä»·å›¢é˜Ÿæ•°: {len(llm_df_filtered)} ä¸ª")

    # åˆå¹¶æ•°æ®
    comparison_data = []

    for _, human_row in human_df.iterrows():
        team = human_row["team_normalized"]

        # æŸ¥æ‰¾å¯¹åº”çš„æœºå™¨è¯„ä»·å’ŒLLMè¯„ä»·
        machine_row = machine_df_filtered[
            machine_df_filtered["team_normalized"] == team
        ]
        llm_row = llm_df_filtered[llm_df_filtered["team_name"] == team]

        comparison_data.append(
            {
                "team_name": team,
                "human_rank": human_row["human_rank"],
                "human_score": human_row["human_score"],
                "machine_rank": (
                    machine_row.iloc[0]["machine_rank"]
                    if len(machine_row) > 0
                    else None
                ),
                "machine_score": (
                    machine_row.iloc[0]["machine_score"]
                    if len(machine_row) > 0
                    else None
                ),
                "llm_rank": llm_row.iloc[0]["llm_rank"] if len(llm_row) > 0 else None,
                "llm_score": (
                    llm_row.iloc[0]["j_score_mean"] if len(llm_row) > 0 else None
                ),
            }
        )

    comparison_df = pd.DataFrame(comparison_data)

    # æŒ‰äººå·¥æ’åæ’åºæ˜¾ç¤º
    comparison_df = comparison_df.sort_values("human_rank")

    print(f"\nğŸ“Š è¯¦ç»†å¯¹æ¯” (å…± {len(comparison_df)} ä¸ªå›¢é˜Ÿ)")
    print("-" * 100)
    print(
        f"{'å›¢é˜Ÿåç§°':<25} {'äººå·¥æ’å':<8} {'äººå·¥åˆ†æ•°':<8} {'æœºå™¨æ’å':<8} {'æœºå™¨åˆ†æ•°':<8} {'LLMæ’å':<8} {'LLMåˆ†æ•°':<8}"
    )
    print("-" * 100)

    for _, row in comparison_df.iterrows():
        machine_rank_str = (
            str(int(row["machine_rank"])) if pd.notna(row["machine_rank"]) else "N/A"
        )
        machine_score_str = (
            f"{row['machine_score']:.3f}" if pd.notna(row["machine_score"]) else "N/A"
        )
        llm_rank_str = str(int(row["llm_rank"])) if pd.notna(row["llm_rank"]) else "N/A"
        llm_score_str = (
            f"{row['llm_score']:.3f}" if pd.notna(row["llm_score"]) else "N/A"
        )

        print(
            f"{row['team_name']:<25} {row['human_rank']:<8} {row['human_score']:<8.2f} "
            f"{machine_rank_str:<8} {machine_score_str:<8} "
            f"{llm_rank_str:<8} {llm_score_str:<8}"
        )

    # ä¿å­˜ç»“æœ
    output_dir = os.path.join("data", "evaluation_results", "llm_evaluation")
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, "all_evaluations_comparison_zh.csv")
    comparison_df.to_csv(output_file, index=False)
    print(f"\nğŸ“ ä¸­æ–‡å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # æ£€æŸ¥æ˜¯å¦å¡«å…¥äº†å®é™…åˆ†æ•°
    has_human_scores = any(row["human_score"] > 0 for _, row in human_df.iterrows())
    has_machine_scores = any(row["machine_score"] > 0 for _, row in machine_df.iterrows())

    if not has_human_scores or not has_machine_scores:
        print(f"\nâš ï¸  æ³¨æ„ï¼šè¯·åœ¨è„šæœ¬ä¸­å¡«å…¥å®é™…çš„äººå·¥è¯„ä»·å’Œæœºå™¨è¯„ä»·åˆ†æ•°åé‡æ–°è¿è¡Œ")
        print(f"   - äººå·¥è¯„ä»·åˆ†æ•°ï¼šç¬¬21-36è¡Œçš„ human_score å­—æ®µ")
        print(f"   - æœºå™¨è¯„ä»·åˆ†æ•°ï¼šç¬¬39-54è¡Œçš„ machine_score å­—æ®µ")
        return

    # è¯¦ç»†ç›¸å…³æ€§åˆ†æ
    print(f"\nğŸ”— è¯¦ç»†ç›¸å…³æ€§åˆ†æ:")
    print("=" * 80)

    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    valid_machine_df = comparison_df.dropna(subset=["machine_score"])
    valid_llm_df = comparison_df.dropna(subset=["llm_score"])
    valid_all_df = comparison_df.dropna(subset=["machine_score", "llm_score"])

    def calculate_correlations(x, y, name1, name2, n_samples):
        """è®¡ç®—å¤šç§ç›¸å…³æ€§æŒ‡æ ‡"""
        if len(x) < 2:
            print(f"   {name1} vs {name2}: æ•°æ®ä¸è¶³ (åªæœ‰ {len(x)} ä¸ªæ ·æœ¬)")
            return

        # Pearsonç›¸å…³ç³»æ•° (çº¿æ€§ç›¸å…³)
        pearson_r, pearson_p = pearsonr(x, y)

        # Spearmanç­‰çº§ç›¸å…³ç³»æ•° (å•è°ƒç›¸å…³)
        spearman_r, spearman_p = spearmanr(x, y)

        # Kendall's tau (æ’åä¸€è‡´æ€§)
        kendall_tau, kendall_p = kendalltau(x, y)

        print(f"\n   ğŸ“Š {name1} vs {name2} (åŸºäº {n_samples} ä¸ªå›¢é˜Ÿ):")
        print(f"      â€¢ Pearsonç›¸å…³ç³»æ•°:  {pearson_r:.3f} (p={pearson_p:.3f})")
        print(f"      â€¢ Spearmanç­‰çº§ç›¸å…³: {spearman_r:.3f} (p={spearman_p:.3f})")
        print(f"      â€¢ Kendall's tau:   {kendall_tau:.3f} (p={kendall_p:.3f})")

        # è§£é‡Šç›¸å…³æ€§å¼ºåº¦
        def interpret_correlation(r):
            abs_r = abs(r)
            if abs_r >= 0.9:
                return "éå¸¸å¼º"
            elif abs_r >= 0.7:
                return "å¼º"
            elif abs_r >= 0.5:
                return "ä¸­ç­‰"
            elif abs_r >= 0.3:
                return "å¼±"
            else:
                return "å¾ˆå¼±"

        print(f"      â€¢ ç›¸å…³æ€§å¼ºåº¦: {interpret_correlation(spearman_r)}")

        # è®¡ç®—é¢„æµ‹è¯¯å·®
        mae = mean_absolute_error(x, y)
        rmse = np.sqrt(mean_squared_error(x, y))
        print(f"      â€¢ å¹³å‡ç»å¯¹è¯¯å·®(MAE): {mae:.3f}")
        print(f"      â€¢ å‡æ–¹æ ¹è¯¯å·®(RMSE): {rmse:.3f}")

    # äººå·¥ vs æœºå™¨è¯„ä»·
    if len(valid_machine_df) > 1:
        calculate_correlations(
            valid_machine_df["human_score"],
            valid_machine_df["machine_score"],
            "äººå·¥è¯„ä»·",
            "æœºå™¨è¯„ä»·",
            len(valid_machine_df),
        )

    # äººå·¥ vs LLMè¯„ä»·
    if len(valid_llm_df) > 1:
        calculate_correlations(
            valid_llm_df["human_score"],
            valid_llm_df["llm_score"],
            "äººå·¥è¯„ä»·",
            "LLMè¯„ä»·",
            len(valid_llm_df),
        )

    # æœºå™¨ vs LLMè¯„ä»·
    if len(valid_all_df) > 1:
        calculate_correlations(
            valid_all_df["machine_score"],
            valid_all_df["llm_score"],
            "æœºå™¨è¯„ä»·",
            "LLMè¯„ä»·",
            len(valid_all_df),
        )

    # å‰5åå¯¹æ¯”
    print(f"\nğŸ† å‰5åå¯¹æ¯”:")

    print("\näººå·¥è¯„ä»·å‰5å:")
    top5_human = comparison_df.nsmallest(5, "human_rank")
    for i, (_, row) in enumerate(top5_human.iterrows(), 1):
        print(f"   {i}. {row['team_name']:<25} (åˆ†æ•°: {row['human_score']:.2f})")

    print("\næœºå™¨è¯„ä»·å‰5å:")
    valid_machine_for_top5 = comparison_df.dropna(subset=["machine_rank"])
    if len(valid_machine_for_top5) >= 5:
        top5_machine = valid_machine_for_top5.nsmallest(5, "machine_rank")
        for i, (_, row) in enumerate(top5_machine.iterrows(), 1):
            print(f"   {i}. {row['team_name']:<25} (åˆ†æ•°: {row['machine_score']:.3f})")
    else:
        print(f"   æœºå™¨è¯„ä»·æ•°æ®ä¸è¶³ï¼Œåªæœ‰ {len(valid_machine_for_top5)} ä¸ªå›¢é˜Ÿ")

    print("\nLLMè¯„ä»·å‰5å:")
    valid_llm_for_top5 = comparison_df.dropna(subset=["llm_rank"])
    if len(valid_llm_for_top5) >= 5:
        top5_llm = valid_llm_for_top5.nsmallest(5, "llm_rank")
        for i, (_, row) in enumerate(top5_llm.iterrows(), 1):
            print(f"   {i}. {row['team_name']:<25} (åˆ†æ•°: {row['llm_score']:.3f})")
    else:
        print(f"   LLMè¯„ä»·æ•°æ®ä¸è¶³ï¼Œåªæœ‰ {len(valid_llm_for_top5)} ä¸ªå›¢é˜Ÿ")

    # æ˜¾ç¤ºæ•°æ®å®Œæ•´æ€§ç»Ÿè®¡
    print(f"\nğŸ“Š æ•°æ®å®Œæ•´æ€§ç»Ÿè®¡:")
    print("=" * 50)
    print(f"   - æ€»å›¢é˜Ÿæ•° (äººå·¥è¯„ä»·): {len(comparison_df)}")
    print(
        f"   - æœ‰æœºå™¨è¯„ä»·çš„å›¢é˜Ÿ: {len(comparison_df.dropna(subset=['machine_rank']))}"
    )
    print(f"   - æœ‰LLMè¯„ä»·çš„å›¢é˜Ÿ: {len(comparison_df.dropna(subset=['llm_rank']))}")
    print(
        f"   - ä¸‰ç§è¯„ä»·éƒ½æœ‰çš„å›¢é˜Ÿ: {len(comparison_df.dropna(subset=['machine_rank', 'llm_rank']))}"
    )


if __name__ == "__main__":
    compare_all_evaluations_zh()
