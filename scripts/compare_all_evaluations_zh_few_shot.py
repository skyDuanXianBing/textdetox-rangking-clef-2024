#!/usr/bin/env python3
"""
å¯¹æ¯”ä¸­æ–‡few-shotäººå·¥è¯„ä»·ã€æœºå™¨è¯„ä»·å’ŒLLMè¯„ä»·çš„ç»“æœ
åŸºäºfew-shotè¯„ä»·ç»“æœè¿›è¡Œå¯¹æ¯”åˆ†æ
"""
import pandas as pd
import numpy as np
import os
from scipy.stats import kendalltau, spearmanr, pearsonr
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings

warnings.filterwarnings("ignore")


def compare_all_evaluations_zh_few_shot():
    """å¯¹æ¯”ä¸­æ–‡few-shotå››ç§è¯„ä»·ç»“æœï¼šäººå·¥ã€æœºå™¨ã€Qwen LLMã€DeepSeek LLMã€OpenAI LLMï¼ˆåªåˆ†æä¸­æ–‡çš„15ä¸ªé˜Ÿä¼ï¼‰"""

    print("ğŸ” ä¸­æ–‡Few-Shotè¯„ä»·æ–¹æ³•å¯¹æ¯”åˆ†æï¼ˆåŒ…å«å¤šä¸ªLLMæ¨¡å‹ï¼‰")
    print("=" * 100)

    # ä½¿ç”¨ä¸zero-shotè„šæœ¬ç›¸åŒçš„ä¸­æ–‡äººå·¥è¯„ä»·å’Œæœºå™¨è¯„ä»·æ•°æ®
    # äººå·¥è¯„ä»·æ•°æ®ï¼ˆä¸­æ–‡åˆ†æ•°ï¼Œæ¥è‡ª"Test Phase: Manual Evaluation Final Results"è¡¨çš„ zh åˆ—ï¼‰
    human_data = [
        {"team": "SomethingAwful", "human_score": 0.53},
        {"team": "VitalyProtasov", "human_score": 0.49},
        {"team": "nikita.sushko", "human_score": 0.47},
        {"team": "erehulka", "human_score": 0.68},
        {"team": "Team NLPunks", "human_score": 0.60},
        {"team": "mkrisnai", "human_score": 0.34},
        {"team": "Team cake", "human_score": 0.84},
        {"team": "Team SINAI", "human_score": 0.33},
        {"team": "gleb.shnshn", "human_score": 0.41},
        {"team": "delete", "human_score": 0.43},
        {"team": "mT5", "human_score": 0.43},
        {"team": "Team nlp_enjoyers", "human_score": 0.23},
        {"team": "Team Iron Autobots", "human_score": 0.53},
        {"team": "ZhongyuLuo", "human_score": 0.56},
        {"team": "backtranslation", "human_score": 0.34},
    ]

    # æœºå™¨è¯„ä»·æ•°æ®ï¼ˆä¸­æ–‡åˆ†æ•°ï¼Œæ¥è‡ª"Test Phase: Automatic Evaluation Results"è¡¨çš„ zh åˆ—ï¼‰
    machine_data = [
        {"team": "nikita.sushko", "machine_score": 0.176},
        {"team": "VitalyProtasov", "machine_score": 0.175},
        {"team": "erehulka", "machine_score": 0.160},
        {"team": "SomethingAwful", "machine_score": 0.147},
        {"team": "mkrisnai", "machine_score": 0.109},
        {"team": "Team NLPunks", "machine_score": 0.150},
        {"team": "gleb.shnshn", "machine_score": 0.155},
        {"team": "Team cake", "machine_score": 0.086},
        {"team": "mT5", "machine_score": 0.096},
        {"team": "Team nlp_enjoyers", "machine_score": 0.104},
        {"team": "Team SINAI", "machine_score": 0.126},
        {"team": "delete", "machine_score": 0.175},
        {"team": "Team Iron Autobots", "machine_score": 0.124},
        {"team": "ZhongyuLuo", "machine_score": 0.052},
        {"team": "backtranslation", "machine_score": 0.027},
    ]

    # åˆ›å»ºDataFrame
    human_df = pd.DataFrame(human_data)
    machine_df = pd.DataFrame(machine_data)

    # å›¢é˜Ÿåç§°æ˜ å°„
    name_mapping = {
        "Team SINAI": "Team_SINAI",
        "delete": "delete_baseline",
        "mT5": "mt5_baseline",
        "backtranslation": "backtranslation_baseline",
    }

    # åº”ç”¨åç§°æ˜ å°„
    human_df["team_normalized"] = (
        human_df["team"].map(name_mapping).fillna(human_df["team"])
    )
    machine_df["team_normalized"] = (
        machine_df["team"].map(name_mapping).fillna(machine_df["team"])
    )

    # è®¡ç®—æ’å
    human_df = human_df.sort_values("human_score", ascending=False).reset_index(
        drop=True
    )
    human_df["human_rank"] = human_df.index + 1

    machine_df = machine_df.sort_values("machine_score", ascending=False).reset_index(
        drop=True
    )
    machine_df["machine_rank"] = machine_df.index + 1

    # è¯»å–ä¸­æ–‡LLM Few-Shotè¯„ä»·ç»“æœ
    llm_qwen_file = os.path.join("data", "ranking_results", "zh", "llm_evaluation_ranking_few_shot_qwen.csv")
    llm_deepseek_file = os.path.join("data", "ranking_results", "zh", "llm_evaluation_ranking_few_shot_deepseek.csv")
    llm_openai_file = os.path.join("data", "ranking_results", "zh", "llm_evaluation_ranking_few_shot_openai.csv")

    if not os.path.exists(llm_qwen_file):
        print("âŒ æœªæ‰¾åˆ°ä¸­æ–‡Qwen LLM Few-Shotè¯„ä»·ç»“æœæ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ: python scripts/generate_zh_llm_ranking_few_shot.py")
        return

    if not os.path.exists(llm_deepseek_file):
        print("âŒ æœªæ‰¾åˆ°ä¸­æ–‡DeepSeek LLM Few-Shotè¯„ä»·ç»“æœæ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ: python scripts/generate_zh_llm_ranking_deepseek_few_shot.py")
        return

    if not os.path.exists(llm_openai_file):
        print("âŒ æœªæ‰¾åˆ°ä¸­æ–‡OpenAI LLM Few-Shotè¯„ä»·ç»“æœæ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ: python scripts/generate_zh_llm_ranking_openai.py")
        return

    llm_qwen_df = pd.read_csv(llm_qwen_file)
    llm_deepseek_df = pd.read_csv(llm_deepseek_file)
    llm_openai_df = pd.read_csv(llm_openai_file)

    # LLMåç§°æ˜ å°„ï¼ˆå¤„ç†åç§°ä¸ä¸€è‡´é—®é¢˜ï¼‰
    llm_name_mapping = {"Team nlpjoyers": "Team nlp_enjoyers"}

    # åº”ç”¨LLMåç§°æ˜ å°„
    llm_qwen_df["team_name"] = (
        llm_qwen_df["team_name"].map(llm_name_mapping).fillna(llm_qwen_df["team_name"])
    )
    llm_deepseek_df["team_name"] = (
        llm_deepseek_df["team_name"].map(llm_name_mapping).fillna(llm_deepseek_df["team_name"])
    )
    llm_openai_df["team_name"] = (
        llm_openai_df["team_name"].map(llm_name_mapping).fillna(llm_openai_df["team_name"])
    )

    # è·å–æœ‰äººå·¥è¯„ä»·çš„å›¢é˜Ÿ
    human_teams = set(human_df["team_normalized"])

    print(f"äººå·¥è¯„ä»·å›¢é˜Ÿæ•°: {len(human_teams)} ä¸ª")

    # è¿‡æ»¤å„ç§è¯„ä»·ï¼Œåªä¿ç•™æœ‰äººå·¥è¯„ä»·çš„å›¢é˜Ÿ
    machine_df_filtered = machine_df[machine_df["team_normalized"].isin(human_teams)].copy()
    llm_qwen_df_filtered = llm_qwen_df[llm_qwen_df["team_name"].isin(human_teams)].copy()
    llm_deepseek_df_filtered = llm_deepseek_df[llm_deepseek_df["team_name"].isin(human_teams)].copy()
    llm_openai_df_filtered = llm_openai_df[llm_openai_df["team_name"].isin(human_teams)].copy()

    # é‡æ–°è®¡ç®—æ’åï¼ˆåªé’ˆå¯¹æœ‰äººå·¥è¯„ä»·çš„å›¢é˜Ÿï¼‰
    machine_df_filtered = machine_df_filtered.sort_values("machine_score", ascending=False).reset_index(drop=True)
    machine_df_filtered["machine_rank"] = machine_df_filtered.index + 1

    llm_qwen_df_filtered = llm_qwen_df_filtered.sort_values("j_score_mean", ascending=False).reset_index(drop=True)
    llm_qwen_df_filtered["llm_qwen_rank"] = llm_qwen_df_filtered.index + 1

    llm_deepseek_df_filtered = llm_deepseek_df_filtered.sort_values("j_score_mean", ascending=False).reset_index(drop=True)
    llm_deepseek_df_filtered["llm_deepseek_rank"] = llm_deepseek_df_filtered.index + 1

    llm_openai_df_filtered = llm_openai_df_filtered.sort_values("j_score_mean", ascending=False).reset_index(drop=True)
    llm_openai_df_filtered["llm_openai_rank"] = llm_openai_df_filtered.index + 1

    print(f"è¿‡æ»¤åæœºå™¨è¯„ä»·å›¢é˜Ÿæ•°: {len(machine_df_filtered)} ä¸ª")
    print(f"è¿‡æ»¤åQwen LLMè¯„ä»·å›¢é˜Ÿæ•°: {len(llm_qwen_df_filtered)} ä¸ª")
    print(f"è¿‡æ»¤åDeepSeek LLMè¯„ä»·å›¢é˜Ÿæ•°: {len(llm_deepseek_df_filtered)} ä¸ª")
    print(f"è¿‡æ»¤åOpenAI LLMè¯„ä»·å›¢é˜Ÿæ•°: {len(llm_openai_df_filtered)} ä¸ª")

    print(f"ğŸ” äº”ç§è¯„ä»·æ–¹æ³•å¯¹æ¯”åˆ†æï¼ˆä¸­æ–‡Few-Shotæ•°æ®ï¼Œä»¥äººå·¥è¯„ä»·ä¸ºå‡†ï¼Œå…±{len(human_df)}ä¸ªå›¢é˜Ÿï¼‰")
    print("=" * 160)

    # åˆå¹¶æ•°æ®
    comparison_data = []

    for _, human_row in human_df.iterrows():
        team = human_row["team_normalized"]

        # æŸ¥æ‰¾å¯¹åº”çš„å„ç§è¯„ä»·
        machine_row = machine_df_filtered[machine_df_filtered["team_normalized"] == team]
        llm_qwen_row = llm_qwen_df_filtered[llm_qwen_df_filtered["team_name"] == team]
        llm_deepseek_row = llm_deepseek_df_filtered[llm_deepseek_df_filtered["team_name"] == team]
        llm_openai_row = llm_openai_df_filtered[llm_openai_df_filtered["team_name"] == team]

        comparison_data.append({
            "team_name": team,
            "human_rank": human_row["human_rank"],
            "human_score": human_row["human_score"],
            "machine_rank": machine_row.iloc[0]["machine_rank"] if len(machine_row) > 0 else None,
            "machine_score": machine_row.iloc[0]["machine_score"] if len(machine_row) > 0 else None,
            "llm_qwen_rank": llm_qwen_row.iloc[0]["llm_qwen_rank"] if len(llm_qwen_row) > 0 else None,
            "llm_qwen_score": llm_qwen_row.iloc[0]["j_score_mean"] if len(llm_qwen_row) > 0 else None,
            "llm_deepseek_rank": llm_deepseek_row.iloc[0]["llm_deepseek_rank"] if len(llm_deepseek_row) > 0 else None,
            "llm_deepseek_score": llm_deepseek_row.iloc[0]["j_score_mean"] if len(llm_deepseek_row) > 0 else None,
            "llm_openai_rank": llm_openai_row.iloc[0]["llm_openai_rank"] if len(llm_openai_row) > 0 else None,
            "llm_openai_score": llm_openai_row.iloc[0]["j_score_mean"] if len(llm_openai_row) > 0 else None,
        })

    comparison_df = pd.DataFrame(comparison_data)
    
    # æŒ‰äººå·¥æ’åæ’åºæ˜¾ç¤º
    comparison_df = comparison_df.sort_values("human_rank")

    # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
    print(f"{'å›¢é˜Ÿåç§°':<25} {'äººå·¥æ’å':<8} {'äººå·¥åˆ†æ•°':<8} {'æœºå™¨æ’å':<8} {'æœºå™¨åˆ†æ•°':<8} {'Qwenæ’å':<8} {'Qwenåˆ†æ•°':<8} {'DeepSeekæ’å':<12} {'DeepSeekåˆ†æ•°':<12} {'OpenAIæ’å':<10} {'OpenAIåˆ†æ•°':<10}")
    print("-" * 160)

    for _, row in comparison_df.iterrows():
        qwen_rank_str = str(int(row["llm_qwen_rank"])) if pd.notna(row["llm_qwen_rank"]) else "N/A"
        qwen_score_str = f"{row['llm_qwen_score']:.4f}" if pd.notna(row["llm_qwen_score"]) else "N/A"

        deepseek_rank_str = str(int(row["llm_deepseek_rank"])) if pd.notna(row["llm_deepseek_rank"]) else "N/A"
        deepseek_score_str = f"{row['llm_deepseek_score']:.4f}" if pd.notna(row["llm_deepseek_score"]) else "N/A"

        openai_rank_str = str(int(row["llm_openai_rank"])) if pd.notna(row["llm_openai_rank"]) else "N/A"
        openai_score_str = f"{row['llm_openai_score']:.4f}" if pd.notna(row["llm_openai_score"]) else "N/A"

        print(f"{row['team_name']:<25} {row['human_rank']:<8} {row['human_score']:<8.2f} "
              f"{row['machine_rank']:<8} {row['machine_score']:<8.3f} "
              f"{qwen_rank_str:<8} {qwen_score_str:<8} "
              f"{deepseek_rank_str:<12} {deepseek_score_str:<12} "
              f"{openai_rank_str:<10} {openai_score_str:<10}")

    # ä¿å­˜ç»“æœ
    output_dir = os.path.join("data", "evaluation_results", "llm_evaluation")
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, "all_evaluations_comparison_zh_few_shot_all_models.csv")
    comparison_df.to_csv(output_file, index=False)
    print(f"\nğŸ“ ä¸­æ–‡Few-Shotå¯¹æ¯”ç»“æœï¼ˆåŒ…å«æ‰€æœ‰æ¨¡å‹ï¼‰å·²ä¿å­˜åˆ°: {output_file}")

    # è¯¦ç»†ç›¸å…³æ€§åˆ†æ
    print(f"\nğŸ”— è¯¦ç»†ç›¸å…³æ€§åˆ†æ:")
    print("=" * 80)

    # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
    valid_machine_df = comparison_df.dropna(subset=["machine_score"])
    valid_llm_qwen_df = comparison_df.dropna(subset=["llm_qwen_score"])
    valid_llm_deepseek_df = comparison_df.dropna(subset=["llm_deepseek_score"])
    valid_llm_openai_df = comparison_df.dropna(subset=["llm_openai_score"])
    valid_all_df = comparison_df.dropna(subset=["machine_score", "llm_qwen_score", "llm_deepseek_score", "llm_openai_score"])

    def interpret_correlation(r):
        """è§£é‡Šç›¸å…³ç³»æ•°å¼ºåº¦"""
        abs_r = abs(r)
        if abs_r >= 0.8:
            return "éå¸¸å¼º"
        elif abs_r >= 0.6:
            return "å¼º"
        elif abs_r >= 0.4:
            return "ä¸­ç­‰"
        elif abs_r >= 0.2:
            return "å¼±"
        else:
            return "å¾ˆå¼±"

    def calculate_detailed_stats(x, y, name1, name2, sample_size):
        """è®¡ç®—å¹¶æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡æŒ‡æ ‡"""
        print(f"\nğŸ“Š {name1} vs {name2} (æ ·æœ¬æ•°: {sample_size})")
        print("-" * 60)

        # åŸºæœ¬ç»Ÿè®¡
        x_mean, y_mean = np.mean(x), np.mean(y)
        x_std, y_std = np.std(x), np.std(y)
        x_var, y_var = np.var(x), np.var(y)

        print(f"      â€¢ åŸºæœ¬ç»Ÿè®¡:")
        print(f"        - {name1}: å‡å€¼={x_mean:.3f}, æ ‡å‡†å·®={x_std:.3f}")
        print(f"        - {name2}: å‡å€¼={y_mean:.3f}, æ ‡å‡†å·®={y_std:.3f}")

        # ç›¸å…³æ€§åˆ†æ
        pearson_r, pearson_p = pearsonr(x, y)
        spearman_r, spearman_p = spearmanr(x, y)
        kendall_tau, kendall_p = kendalltau(x, y)

        print(f"      â€¢ ç›¸å…³æ€§åˆ†æ:")
        print(f"        - Pearsonç›¸å…³ç³»æ•°: {pearson_r:.3f} (p={pearson_p:.3f})")
        print(f"        - Spearmanç›¸å…³ç³»æ•°: {spearman_r:.3f} (p={spearman_p:.3f})")
        print(f"        - Kendall's tau: {kendall_tau:.3f} (p={kendall_p:.3f})")
        print(f"        - ç›¸å…³æ€§å¼ºåº¦: {interpret_correlation(spearman_r)}")

        # é¢„æµ‹è¯¯å·®
        mae = mean_absolute_error(x, y)
        rmse = np.sqrt(mean_squared_error(x, y))
        print(f"        - å¹³å‡ç»å¯¹è¯¯å·®(MAE): {mae:.3f}")
        print(f"        - å‡æ–¹æ ¹è¯¯å·®(RMSE): {rmse:.3f}")

        # æ–¹å·®æ¯”è¾ƒ
        var_ratio = x_var / y_var if y_var > 0 else float('inf')
        if var_ratio > 1.5:
            print(f"        - æ–¹å·®æ¯”è¾ƒ: {name1}å˜å¼‚æ€§æ›´å¤§ (æ¯”å€¼: {var_ratio:.2f})")
        elif var_ratio < 0.67:
            print(f"        - æ–¹å·®æ¯”è¾ƒ: {name2}å˜å¼‚æ€§æ›´å¤§ (æ¯”å€¼: {var_ratio:.2f})")
        else:
            print(f"        - æ–¹å·®æ¯”è¾ƒ: å˜å¼‚æ€§ç›¸è¿‘ (æ¯”å€¼: {var_ratio:.2f})")

        # ç»å¯¹å€¼åˆ†æ
        print(f"      â€¢ ç»å¯¹å€¼åˆ†æ:")
        diff = np.array(x) - np.array(y)
        abs_diff = np.abs(diff)
        mean_abs_diff = np.mean(abs_diff)
        median_abs_diff = np.median(abs_diff)
        max_abs_diff = np.max(abs_diff)
        min_abs_diff = np.min(abs_diff)

        # æŒ‰ç…§å…¬å¼(1)å’Œ(2)è®¡ç®— abs å’Œ var æŒ‡æ ‡
        # å…¬å¼(1): abs = (1/N) * Î£|JS_MER - JS_M| ï¼ˆä¸å¹³å‡ç»å¯¹å·®å€¼ç›¸åŒï¼‰
        print(f"        - abs (å…¬å¼1): {mean_abs_diff:.3f}")
        
        # å…¬å¼(2): var = (1/N) * Î£|JS_MER - JS_M|Â²  
        var_formula = np.mean((diff) ** 2)
        print(f"        - var (å…¬å¼2): {var_formula:.3f}")
        
        print(f"        - å¹³å‡ç»å¯¹å·®å€¼: {mean_abs_diff:.3f}")
        print(f"        - ä¸­ä½æ•°ç»å¯¹å·®å€¼: {median_abs_diff:.3f}")
        print(f"        - æœ€å¤§ç»å¯¹å·®å€¼: {max_abs_diff:.3f}")
        print(f"        - æœ€å°ç»å¯¹å·®å€¼: {min_abs_diff:.3f}")

        # ç»å¯¹å·®å€¼åˆ†å¸ƒ
        small_diff_count = np.sum(abs_diff <= 0.1)
        medium_diff_count = np.sum((abs_diff > 0.1) & (abs_diff <= 0.2))
        large_diff_count = np.sum(abs_diff > 0.2)

        print(f"        - ç»å¯¹å·®å€¼åˆ†å¸ƒ:")
        print(f"          * å°å·®å¼‚(â‰¤0.1): {small_diff_count} ä¸ª ({small_diff_count/len(abs_diff)*100:.1f}%)")
        print(f"          * ä¸­ç­‰å·®å¼‚(0.1-0.2): {medium_diff_count} ä¸ª ({medium_diff_count/len(abs_diff)*100:.1f}%)")
        print(f"          * å¤§å·®å¼‚(>0.2): {large_diff_count} ä¸ª ({large_diff_count/len(abs_diff)*100:.1f}%)")

        # ä¸€è‡´æ€§æŒ‡æ ‡
        consistency_threshold = 0.1  # å¯è°ƒæ•´çš„ä¸€è‡´æ€§é˜ˆå€¼
        consistency_rate = np.sum(abs_diff <= consistency_threshold) / len(abs_diff)
        print(f"        - ä¸€è‡´æ€§ç‡(å·®å¼‚â‰¤{consistency_threshold}): {consistency_rate:.3f}")

        # å¼‚å¸¸å€¼æ£€æµ‹
        q75, q25 = np.percentile(abs_diff, [75, 25])
        iqr = q75 - q25
        outlier_threshold = q75 + 1.5 * iqr
        outliers = abs_diff > outlier_threshold
        outlier_count = np.sum(outliers)

        if outlier_count > 0:
            print(f"        - å¼‚å¸¸å·®å€¼: {outlier_count} ä¸ª (é˜ˆå€¼: {outlier_threshold:.3f})")
            if outlier_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªå¼‚å¸¸å€¼
                outlier_indices = np.where(outliers)[0]
                for idx in outlier_indices[:3]:
                    print(f"          * ç´¢å¼•{idx}: å·®å€¼ {abs_diff[idx]:.3f}")
        else:
            print(f"        - å¼‚å¸¸å·®å€¼: æ—  (é˜ˆå€¼: {outlier_threshold:.3f})")

    # äººå·¥ vs æœºå™¨è¯„ä»·
    if len(valid_machine_df) > 1:
        calculate_detailed_stats(
            valid_machine_df["human_score"],
            valid_machine_df["machine_score"],
            "äººå·¥è¯„ä»·",
            "æœºå™¨è¯„ä»·",
            len(valid_machine_df),
        )

    # äººå·¥ vs å„ä¸ªLLMè¯„ä»·
    if len(valid_llm_qwen_df) > 1:
        calculate_detailed_stats(
            valid_llm_qwen_df["human_score"],
            valid_llm_qwen_df["llm_qwen_score"],
            "äººå·¥è¯„ä»·",
            "Qwen LLMè¯„ä»·",
            len(valid_llm_qwen_df),
        )

    if len(valid_llm_deepseek_df) > 1:
        calculate_detailed_stats(
            valid_llm_deepseek_df["human_score"],
            valid_llm_deepseek_df["llm_deepseek_score"],
            "äººå·¥è¯„ä»·",
            "DeepSeek LLMè¯„ä»·",
            len(valid_llm_deepseek_df),
        )

    if len(valid_llm_openai_df) > 1:
        calculate_detailed_stats(
            valid_llm_openai_df["human_score"],
            valid_llm_openai_df["llm_openai_score"],
            "äººå·¥è¯„ä»·",
            "OpenAI LLMè¯„ä»·",
            len(valid_llm_openai_df),
        )

    # æœºå™¨ vs å„ä¸ªLLMè¯„ä»·
    if len(valid_all_df) > 1:
        calculate_detailed_stats(
            valid_all_df["machine_score"],
            valid_all_df["llm_qwen_score"],
            "æœºå™¨è¯„ä»·",
            "Qwen LLMè¯„ä»·",
            len(valid_all_df),
        )

        calculate_detailed_stats(
            valid_all_df["machine_score"],
            valid_all_df["llm_deepseek_score"],
            "æœºå™¨è¯„ä»·",
            "DeepSeek LLMè¯„ä»·",
            len(valid_all_df),
        )

        calculate_detailed_stats(
            valid_all_df["machine_score"],
            valid_all_df["llm_openai_score"],
            "æœºå™¨è¯„ä»·",
            "OpenAI LLMè¯„ä»·",
            len(valid_all_df),
        )

        # LLMä¹‹é—´çš„å¯¹æ¯”
        calculate_detailed_stats(
            valid_all_df["llm_qwen_score"],
            valid_all_df["llm_deepseek_score"],
            "Qwen LLMè¯„ä»·",
            "DeepSeek LLMè¯„ä»·",
            len(valid_all_df),
        )

        calculate_detailed_stats(
            valid_all_df["llm_qwen_score"],
            valid_all_df["llm_openai_score"],
            "Qwen LLMè¯„ä»·",
            "OpenAI LLMè¯„ä»·",
            len(valid_all_df),
        )

        calculate_detailed_stats(
            valid_all_df["llm_deepseek_score"],
            valid_all_df["llm_openai_score"],
            "DeepSeek LLMè¯„ä»·",
            "OpenAI LLMè¯„ä»·",
            len(valid_all_df),
        )

    # æ˜¾ç¤ºå‰5åå¯¹æ¯”
    print(f"\nğŸ† å„è¯„ä»·æ–¹æ³•å‰5åå¯¹æ¯”:")
    print("=" * 80)

    print("äººå·¥è¯„ä»·å‰5å:")
    for i, (_, row) in enumerate(comparison_df.head(5).iterrows(), 1):
        print(f"   {i}. {row['team_name']:<25} (åˆ†æ•°: {row['human_score']:.3f})")

    print("\næœºå™¨è¯„ä»·å‰5å:")
    valid_machine_for_top5 = comparison_df.dropna(subset=["machine_rank"])
    if len(valid_machine_for_top5) >= 5:
        top5_machine = valid_machine_for_top5.nsmallest(5, "machine_rank")
        for i, (_, row) in enumerate(top5_machine.iterrows(), 1):
            print(f"   {i}. {row['team_name']:<25} (åˆ†æ•°: {row['machine_score']:.3f})")
    else:
        print(f"   æœºå™¨è¯„ä»·æ•°æ®ä¸è¶³ï¼Œåªæœ‰ {len(valid_machine_for_top5)} ä¸ªå›¢é˜Ÿ")

    print("\nQwen LLMè¯„ä»·å‰5å:")
    valid_qwen_for_top5 = comparison_df.dropna(subset=["llm_qwen_rank"])
    if len(valid_qwen_for_top5) >= 5:
        top5_qwen = valid_qwen_for_top5.nsmallest(5, "llm_qwen_rank")
        for i, (_, row) in enumerate(top5_qwen.iterrows(), 1):
            print(f"   {i}. {row['team_name']:<25} (åˆ†æ•°: {row['llm_qwen_score']:.4f})")
    else:
        print(f"   Qwen LLMè¯„ä»·æ•°æ®ä¸è¶³ï¼Œåªæœ‰ {len(valid_qwen_for_top5)} ä¸ªå›¢é˜Ÿ")

    print("\nDeepSeek LLMè¯„ä»·å‰5å:")
    valid_deepseek_for_top5 = comparison_df.dropna(subset=["llm_deepseek_rank"])
    if len(valid_deepseek_for_top5) >= 5:
        top5_deepseek = valid_deepseek_for_top5.nsmallest(5, "llm_deepseek_rank")
        for i, (_, row) in enumerate(top5_deepseek.iterrows(), 1):
            print(f"   {i}. {row['team_name']:<25} (åˆ†æ•°: {row['llm_deepseek_score']:.4f})")
    else:
        print(f"   DeepSeek LLMè¯„ä»·æ•°æ®ä¸è¶³ï¼Œåªæœ‰ {len(valid_deepseek_for_top5)} ä¸ªå›¢é˜Ÿ")

    print("\nOpenAI LLMè¯„ä»·å‰5å:")
    valid_openai_for_top5 = comparison_df.dropna(subset=["llm_openai_rank"])
    if len(valid_openai_for_top5) >= 5:
        top5_openai = valid_openai_for_top5.nsmallest(5, "llm_openai_rank")
        for i, (_, row) in enumerate(top5_openai.iterrows(), 1):
            print(f"   {i}. {row['team_name']:<25} (åˆ†æ•°: {row['llm_openai_score']:.4f})")
    else:
        print(f"   OpenAI LLMè¯„ä»·æ•°æ®ä¸è¶³ï¼Œåªæœ‰ {len(valid_openai_for_top5)} ä¸ªå›¢é˜Ÿ")


    # æ˜¾ç¤ºæ•°æ®å®Œæ•´æ€§ç»Ÿè®¡
    print(f"\nğŸ“Š æ•°æ®å®Œæ•´æ€§ç»Ÿè®¡:")
    print("=" * 50)
    print(f"   - æ€»å›¢é˜Ÿæ•° (äººå·¥è¯„ä»·): {len(comparison_df)}")
    print(f"   - æœ‰æœºå™¨è¯„ä»·çš„å›¢é˜Ÿ: {len(comparison_df.dropna(subset=['machine_rank']))}")
    print(f"   - æœ‰Qwen LLMè¯„ä»·çš„å›¢é˜Ÿ: {len(comparison_df.dropna(subset=['llm_qwen_rank']))}")
    print(f"   - æœ‰DeepSeek LLMè¯„ä»·çš„å›¢é˜Ÿ: {len(comparison_df.dropna(subset=['llm_deepseek_rank']))}")
    print(f"   - æœ‰OpenAI LLMè¯„ä»·çš„å›¢é˜Ÿ: {len(comparison_df.dropna(subset=['llm_openai_rank']))}")
    print(f"   - äº”ç§è¯„ä»·éƒ½æœ‰çš„å›¢é˜Ÿ: {len(comparison_df.dropna(subset=['machine_rank', 'llm_qwen_rank', 'llm_deepseek_rank', 'llm_openai_rank']))}")

    return comparison_df

if __name__ == "__main__":
    compare_all_evaluations_zh_few_shot()